import gradio as gr
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, TextIteratorStreamer
from qwen_vl_utils import process_vision_info
from PIL import Image, ImageDraw, ImageFont
import random
import os
import warnings
from typing import List, Tuple, Optional, Generator
import gc
import json
import re
import ast
import csv
import shutil
from datetime import datetime
import time
import tempfile
from threading import Thread
import io
import sys
import logging

# Optional: psutil for memory monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("Note: psutil not installed. RAM monitoring will be limited.")


# Suppress meta device warning (not useful)
warnings.filterwarnings('ignore', message='.*meta device.*')

# Global flag for stopping generation
stop_generation_flag = False

# ==========================================
# Console Log Capture for UI display
# ==========================================
class LogCapture:
    """Captures stdout and stderr for real-time console output in UI"""
    def __init__(self):
        self.log_buffer = io.StringIO()
        self.original_stdout = sys.stdout
        self.original_stderr = sys.stderr
        self.is_capturing = False

    def start_capture(self):
        """Start capturing stdout and stderr"""
        if not self.is_capturing:
            sys.stdout = self
            sys.stderr = self
            self.is_capturing = True

    def stop_capture(self):
        """Stop capturing stdout and stderr"""
        if self.is_capturing:
            sys.stdout = self.original_stdout
            sys.stderr = self.original_stderr
            self.is_capturing = False

    def write(self, message):
        """Write to both original stdout/stderr and buffer"""
        self.original_stdout.write(message)
        self.log_buffer.write(message)

    def flush(self):
        """Flush stdout and stderr"""
        self.original_stdout.flush()
        self.original_stderr.flush()

    def get_logs(self):
        """Get captured logs"""
        return self.log_buffer.getvalue()

    def clear_logs(self):
        """Clear log buffer"""
        self.log_buffer = io.StringIO()

# Global log capture instance
log_capture = LogCapture()

# ==========================================
# Thinking Models Utilities
# ==========================================
def parse_thinking_output(text: str) -> Tuple[str, str]:
    """
    Parse thinking model output to separate reasoning from final answer

    Returns:
        Tuple of (reasoning, final_answer)
        If no </think> tag found, returns ("", full_text)
    """
    # Check for </think> tag
    if '</think>' in text:
        parts = text.split('</think>', 1)
        if len(parts) == 2:
            reasoning = parts[0].strip()
            # Remove <think> tag if present at start
            if reasoning.startswith('<think>'):
                reasoning = reasoning[7:].strip()
            final_answer = parts[1].strip()
            return reasoning, final_answer

    # No thinking tag found
    return "", text

# ==========================================
# Visual Grounding Utilities
# ==========================================
def parse_bboxes_from_text(text: str) -> list:
    """
    Parse bounding boxes from model output text
    Supports multiple formats:
    1. JSON format: [{"bbox_2d": [x1, y1, x2, y2], "label": "..."}]
    2. Inline format: {"bbox_2d": [x1, y1, x2, y2], "label": "..."}
    """
    bboxes = []

    # Try JSON format first
    try:
        # Extract JSON from markdown code block if present
        if '```json' in text:
            json_str = text.split('```json')[1].split('```')[0].strip()
        elif '```' in text:
            json_str = text.split('```')[1].split('```')[0].strip()
        else:
            # Try to find JSON array directly
            start = text.find('[')
            end = text.rfind(']') + 1
            if start >= 0 and end > start:
                json_str = text[start:end]
            else:
                # Try single object
                start = text.find('{')
                end = text.rfind('}') + 1
                if start >= 0 and end > start:
                    json_str = '[' + text[start:end] + ']'
                else:
                    return []

        parsed = json.loads(json_str)
        if isinstance(parsed, list):
            bboxes = parsed
        elif isinstance(parsed, dict):
            bboxes = [parsed]
    except (json.JSONDecodeError, ValueError, IndexError):
        # Try ast.literal_eval as fallback
        try:
            # Find all dict-like patterns
            pattern = r'\{[^{}]*"bbox_2d"[^{}]*\}'
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    bbox_dict = ast.literal_eval(match)
                    if 'bbox_2d' in bbox_dict:
                        bboxes.append(bbox_dict)
                except:
                    pass
        except:
            pass

    return bboxes

def draw_bboxes_on_image(image_path: str, bboxes: list, output_path: str = None, normalized: bool = True) -> str:
    """
    Draw bounding boxes on image

    Args:
        image_path: path to image file
        bboxes: list of bbox dicts with 'bbox_2d' and 'label' keys
        output_path: optional path to save the result (if None, creates temp file)
        normalized: if True, bboxes are in [0,1000] range; if False, pixel coords

    Returns:
        Path to output image with drawn boxes
    """
    if not bboxes:
        return image_path

    # Load image
    img = Image.open(image_path)
    if img.mode != 'RGB':
        img = img.convert('RGB')

    draw = ImageDraw.Draw(img)
    img_width, img_height = img.size

    # Try to load a font, fallback to default if not available
    try:
        font = ImageFont.truetype("arial.ttf", 16)
    except:
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        except:
            font = ImageFont.load_default()

    # Draw each bounding box
    for bbox_item in bboxes:
        bbox = bbox_item.get('bbox_2d', [])
        label = bbox_item.get('label', 'unknown')

        if len(bbox) != 4:
            continue

        x1, y1, x2, y2 = bbox

        # Convert from normalized to pixel coordinates if needed
        if normalized:
            x1 = int(x1 / 1000 * img_width)
            y1 = int(y1 / 1000 * img_height)
            x2 = int(x2 / 1000 * img_width)
            y2 = int(y2 / 1000 * img_height)
        else:
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)

        # Draw rectangle
        draw.rectangle([x1, y1, x2, y2], outline='red', width=3)

        # Draw label background
        try:
            bbox_text = draw.textbbox((x1, y1 - 20), label, font=font)
            draw.rectangle(bbox_text, fill='red')
            draw.text((x1, y1 - 20), label, fill='white', font=font)
        except:
            # Fallback without textbbox if not available
            draw.text((x1, y1 - 20), label, fill='red', font=font)

    # Save result
    if output_path is None:
        output_path = os.path.join(TEMP_DIR, f"bbox_{os.path.basename(image_path)}")

    img.save(output_path)
    return output_path

# Base directory for portable app
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRIPT_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output")
DATASETS_DIR = os.path.join(SCRIPT_DIR, "datasets")
PROMPTS_DIR = os.path.join(SCRIPT_DIR, "prompts")

# Create directories if they don't exist
os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DATASETS_DIR, exist_ok=True)
os.makedirs(PROMPTS_DIR, exist_ok=True)

# Huggingface cache directory - check env variables first, then platform-specific defaults
def get_hf_cache_dir():
    """Get HuggingFace cache directory, respecting environment variables and platform defaults"""
    # Check HuggingFace environment variables first
    if os.environ.get("HUGGINGFACE_HUB_CACHE"):
        return os.environ["HUGGINGFACE_HUB_CACHE"]
    if os.environ.get("HF_HOME"):
        return os.path.join(os.environ["HF_HOME"], "hub")
    if os.environ.get("HF_HUB_CACHE"):
        return os.environ["HF_HUB_CACHE"]

    # Platform-specific defaults
    if os.name == 'nt':  # Windows
        # Windows: typically in USERPROFILE\.cache\huggingface\hub
        local_app_data = os.environ.get("LOCALAPPDATA")
        if local_app_data:
            cache_in_localappdata = os.path.join(local_app_data, "huggingface", "hub")
            if os.path.exists(cache_in_localappdata):
                return cache_in_localappdata
        # Fall back to home directory
        return os.path.join(os.path.expanduser("~"), ".cache", "huggingface", "hub")
    else:
        # Linux/Mac
        xdg_cache = os.environ.get("XDG_CACHE_HOME", os.path.join(os.path.expanduser("~"), ".cache"))
        return os.path.join(xdg_cache, "huggingface", "hub")

HF_CACHE_DIR = get_hf_cache_dir()

def get_model_cache_size(model_id: str) -> Optional[str]:
    """Check if model is downloaded and return its size on disk"""
    try:
        # Convert model_id to cache folder name format
        # huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated -> models--huihui-ai--Huihui-Qwen3-VL-2B-Instruct-abliterated
        cache_name = "models--" + model_id.replace("/", "--")
        cache_path = os.path.join(HF_CACHE_DIR, cache_name)

        if os.path.exists(cache_path):
            # Calculate total size
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(cache_path):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    if os.path.exists(fp):
                        total_size += os.path.getsize(fp)

            # Convert to human-readable format
            if total_size >= 1024 ** 3:
                return f"{total_size / (1024 ** 3):.1f}GB"
            elif total_size >= 1024 ** 2:
                return f"{total_size / (1024 ** 2):.0f}MB"
            else:
                return f"{total_size / 1024:.0f}KB"
        return None
    except:
        return None

# Available models (without MOE models which have different architecture)
AVAILABLE_MODELS = [
    # Abliterated models (Ğ±ĞµĞ· Ñ†ĞµĞ½Ğ·ÑƒÑ€Ñ‹) - Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ
    ("2B Instruct Abliterated", "huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated"),
    ("2B Thinking Abliterated", "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated"),
    ("4B Instruct Abliterated", "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated"),
    ("4B Thinking Abliterated", "huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated"),
    ("8B Instruct Abliterated", "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"),
    ("8B Thinking Abliterated", "huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated"),
    ("32B Instruct Abliterated", "huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated"),
    ("32B Thinking Abliterated", "huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated"),
    # Original Qwen models (Ñ Ñ†ĞµĞ½Ğ·ÑƒÑ€Ğ¾Ğ¹)
    ("Qwen 2B Instruct", "Qwen/Qwen3-VL-2B-Instruct"),
    ("Qwen 4B Instruct", "Qwen/Qwen3-VL-4B-Instruct"),
    ("Qwen 8B Instruct", "Qwen/Qwen3-VL-8B-Instruct"),
]

def get_model_choices():
    """Get model choices with download status indicator"""
    choices = []
    for name, model_id in AVAILABLE_MODELS:
        cached_size = get_model_cache_size(model_id)
        if cached_size:
            # Model is downloaded - show size
            display_name = f"âœ… {name} [{cached_size}]"
        else:
            # Model not downloaded - will be downloaded on first use
            display_name = f"â¬‡ï¸ {name} [Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ°]"
        choices.append((display_name, model_id))
    return choices

# Extra options for description enhancement (IMAGE)
EXTRA_OPTIONS = {
    "en": {
        "Include lighting info": "Include information about lighting.",
        "Include camera angle": "Include information about camera angle.",
        "Include watermark info": "Include information about whether there is a watermark or not.",
        "Include JPEG artifacts info": "Include information about whether there are JPEG artifacts or not.",
        "Include camera/photo details": "If it is a photo, include information about what camera was likely used and details such as aperture, shutter speed, ISO, etc.",
        "Keep it SFW/PG": "Do NOT include anything sexual; keep it PG.",
        "Don't mention resolution": "Do NOT mention the image's resolution.",
        "Include aesthetic quality": "Include information about the subjective aesthetic quality of the image from low to very high.",
        "Include composition style": "Include information on the image's composition style, such as leading lines, rule of thirds, or symmetry.",
        "Don't mention text in image": "Do NOT mention any text that is in the image.",
        "Include depth of field": "Specify the depth of field and whether the background is in focus or blurred.",
        "Describe only key elements": "ONLY describe the most important elements of the image."
    },
    "ru": {
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ€Ğ°ĞºÑƒÑ€Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.",
        "Ğ£Ğ¿Ğ¾Ğ¼ÑĞ½ÑƒÑ‚ÑŒ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğº": "Ğ£ĞºĞ°Ğ¶Ğ¸, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğº Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.",
        "Ğ£Ğ¿Ğ¾Ğ¼ÑĞ½ÑƒÑ‚ÑŒ JPEG Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹": "Ğ£ĞºĞ°Ğ¶Ğ¸, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ JPEG Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹/Ñ„Ğ¾Ñ‚Ğ¾": "Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ, ÑƒĞºĞ°Ğ¶Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ°Ğ¼ĞµÑ€Ğµ, Ğ°Ğ¿ĞµÑ€Ñ‚ÑƒÑ€Ğµ, Ğ²Ñ‹Ğ´ĞµÑ€Ğ¶ĞºĞµ, ISO Ğ¸ Ñ‚.Ğ´.",
        "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ SFW/PG Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³": "ĞĞ• Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ¹ ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ PG.",
        "ĞĞµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ": "ĞĞ• ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ñ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸Ğ»ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑ‚Ğ¸Ğ»Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ (Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ñ‚Ñ€ĞµÑ‚ĞµĞ¹, ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ).",
        "ĞĞµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸": "ĞĞ• ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ¹ Ñ‚ĞµĞºÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞµÑÑ‚ÑŒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸": "Ğ£ĞºĞ°Ğ¶Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ğ½Ğ°.",
        "ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹": "ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ ÑĞ°Ğ¼Ñ‹Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
    },
    "zh": {
        "åŒ…å«å…‰ç…§ä¿¡æ¯": "åŒ…å«å…‰ç…§ä¿¡æ¯ã€‚",
        "åŒ…å«ç›¸æœºè§’åº¦": "åŒ…å«ç›¸æœºè§’åº¦ä¿¡æ¯ã€‚",
        "æåŠæ°´å°": "è¯´æ˜å›¾ç‰‡æ˜¯å¦æœ‰æ°´å°ã€‚",
        "æåŠJPEGä¼ªå½±": "è¯´æ˜å›¾ç‰‡æ˜¯å¦æœ‰JPEGä¼ªå½±ã€‚",
        "åŒ…å«ç›¸æœº/ç…§ç‰‡è¯¦æƒ…": "å¦‚æœæ˜¯ç…§ç‰‡ï¼ŒåŒ…å«å¯èƒ½ä½¿ç”¨çš„ç›¸æœºä¿¡æ¯ï¼Œå¦‚å…‰åœˆã€å¿«é—¨é€Ÿåº¦ã€ISOç­‰ã€‚",
        "ä¿æŒSFW/PGçº§åˆ«": "ä¸è¦åŒ…å«ä»»ä½•æ€§ç›¸å…³å†…å®¹ï¼Œä¿æŒPGçº§åˆ«ã€‚",
        "ä¸è¦æåŠåˆ†è¾¨ç‡": "ä¸è¦æåŠå›¾ç‰‡çš„åˆ†è¾¨ç‡ã€‚",
        "åŒ…å«ç¾å­¦è´¨é‡è¯„ä»·": "åŒ…å«ä»ä½åˆ°éå¸¸é«˜çš„ä¸»è§‚ç¾å­¦è´¨é‡è¯„ä»·ã€‚",
        "åŒ…å«æ„å›¾é£æ ¼": "åŒ…å«æ„å›¾é£æ ¼ä¿¡æ¯ï¼Œå¦‚å¼•å¯¼çº¿ã€ä¸‰åˆ†æ³•åˆ™æˆ–å¯¹ç§°æ€§ã€‚",
        "ä¸è¦æåŠå›¾ç‰‡ä¸­çš„æ–‡å­—": "ä¸è¦æåŠå›¾ç‰‡ä¸­çš„ä»»ä½•æ–‡å­—ã€‚",
        "åŒ…å«æ™¯æ·±ä¿¡æ¯": "è¯´æ˜æ™¯æ·±ä»¥åŠèƒŒæ™¯æ˜¯å¦æ¨¡ç³Šã€‚",
        "åªæè¿°å…³é”®å…ƒç´ ": "åªæè¿°å›¾ç‰‡ä¸­æœ€é‡è¦çš„å…ƒç´ ã€‚"
    }
}

# Extra options for VIDEO description enhancement
EXTRA_OPTIONS_VIDEO = {
    "en": {
        "Describe camera movement": "Describe camera movements (panning, zooming, static, etc.).",
        "Include audio description": "If the video has audio, describe it (music, speech, sound effects).",
        "Describe plot/story": "Describe the plot or story progression in the video.",
        "Include timestamps": "Provide timestamps in HH:MM:SS format for key events (e.g., 'person started running at 00:01:23').",
        "Include lighting info": "Include information about lighting changes throughout the video.",
        "Include editing style": "Describe the editing style (cuts, transitions, effects).",
        "Keep it SFW/PG": "Do NOT include anything sexual; keep it PG.",
        "Describe only key moments": "ONLY describe the most important moments in the video.",
        "Include aesthetic quality": "Include information about the subjective aesthetic quality from low to very high."
    },
    "ru": {
        "ĞĞ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹": "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ (Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ·ÑƒĞ¼, ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¸ Ñ‚.Ğ´.).",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ°": "Ğ•ÑĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ ĞµÑÑ‚ÑŒ Ğ·Ğ²ÑƒĞº, Ğ¾Ğ¿Ğ¸ÑˆĞ¸ ĞµĞ³Ğ¾ (Ğ¼ÑƒĞ·Ñ‹ĞºĞ°, Ñ€ĞµÑ‡ÑŒ, Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹).",
        "ĞĞ¿Ğ¸ÑĞ°Ñ‚ÑŒ ÑÑĞ¶ĞµÑ‚/Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ": "ĞĞ¿Ğ¸ÑˆĞ¸ ÑÑĞ¶ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.",
        "Ğ£ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ¹Ğ¼ÑÑ‚Ğ°Ğ¼Ğ¿Ñ‹": "Ğ£ĞºĞ°Ğ¶Ğ¸ Ñ‚Ğ°Ğ¹Ğ¼ÑÑ‚Ğ°Ğ¼Ğ¿Ñ‹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ§Ğ§:ĞœĞœ:Ğ¡Ğ¡ Ğ´Ğ»Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ½Ğ°Ñ‡Ğ°Ğ» Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ² 00:01:23').",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ°": "ĞĞ¿Ğ¸ÑˆĞ¸ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ° (Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹).",
        "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ SFW/PG Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³": "ĞĞ• Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ¹ ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ PG.",
        "ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹": "ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ ÑĞ°Ğ¼Ñ‹Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾.",
        "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ": "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ñ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾."
    },
    "zh": {
        "æè¿°é•œå¤´è¿åŠ¨": "æè¿°é•œå¤´è¿åŠ¨ï¼ˆå¹³ç§»ã€ç¼©æ”¾ã€é™æ­¢ç­‰ï¼‰ã€‚",
        "åŒ…å«éŸ³é¢‘æè¿°": "å¦‚æœè§†é¢‘æœ‰éŸ³é¢‘ï¼Œæè¿°å®ƒï¼ˆéŸ³ä¹ã€è¯­éŸ³ã€éŸ³æ•ˆï¼‰ã€‚",
        "æè¿°æƒ…èŠ‚/æ•…äº‹": "æè¿°è§†é¢‘ä¸­çš„æƒ…èŠ‚æˆ–æ•…äº‹å‘å±•ã€‚",
        "åŒ…å«æ—¶é—´æˆ³": "ä»¥HH:MM:SSæ ¼å¼æä¾›å…³é”®äº‹ä»¶çš„æ—¶é—´æˆ³ï¼ˆä¾‹å¦‚ï¼Œ'äººç‰©åœ¨00:01:23å¼€å§‹å¥”è·‘'ï¼‰ã€‚",
        "åŒ…å«å…‰ç…§ä¿¡æ¯": "åŒ…å«è§†é¢‘ä¸­å…‰ç…§å˜åŒ–çš„ä¿¡æ¯ã€‚",
        "åŒ…å«å‰ªè¾‘é£æ ¼": "æè¿°å‰ªè¾‘é£æ ¼ï¼ˆåˆ‡æ¢ã€è¿‡æ¸¡ã€æ•ˆæœï¼‰ã€‚",
        "ä¿æŒSFW/PGçº§åˆ«": "ä¸è¦åŒ…å«ä»»ä½•æ€§ç›¸å…³å†…å®¹ï¼Œä¿æŒPGçº§åˆ«ã€‚",
        "åªæè¿°å…³é”®æ—¶åˆ»": "åªæè¿°è§†é¢‘ä¸­æœ€é‡è¦çš„æ—¶åˆ»ã€‚",
        "åŒ…å«ç¾å­¦è´¨é‡è¯„ä»·": "åŒ…å«ä»ä½åˆ°éå¸¸é«˜çš„ä¸»è§‚ç¾å­¦è´¨é‡è¯„ä»·ã€‚"
    }
}

def get_memory_info() -> str:
    """Get current memory usage information"""
    info_parts = []

    # GPU memory
    if torch.cuda.is_available():
        try:
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            info_parts.append(f"GPU: {allocated:.1f}/{total:.1f} GB (reserved: {reserved:.1f} GB)")
        except:
            pass

    # RAM
    if PSUTIL_AVAILABLE:
        try:
            process = psutil.Process(os.getpid())
            ram_usage = process.memory_info().rss / (1024 ** 3)
            info_parts.append(f"RAM: {ram_usage:.1f} GB")
        except:
            pass

    return " | ".join(info_parts) if info_parts else "Memory info unavailable"

def get_video_duration(video_path: str) -> float:
    """Get video duration in seconds using ffprobe (lightweight, no extra dependencies)"""
    if not video_path or not os.path.exists(video_path):
        return 7200.0  # Default 2 hours

    # Use ffprobe - fast, lightweight, no loading video into memory
    try:
        import subprocess
        result = subprocess.run(
            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
             '-of', 'default=noprint_wrappers=1:nokey=1', video_path],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            duration = float(result.stdout.strip())
            if duration > 0:
                return duration
    except:
        pass

    # Fallback to 2 hours if ffprobe not available
    return 7200.0

def load_prompt_presets() -> dict:
    """Load prompt presets from the prompts directory"""
    presets = {"None": ""}

    if not os.path.exists(PROMPTS_DIR):
        return presets

    for filename in os.listdir(PROMPTS_DIR):
        if filename.endswith('.txt'):
            preset_name = os.path.splitext(filename)[0]
            try:
                with open(os.path.join(PROMPTS_DIR, filename), 'r', encoding='utf-8') as f:
                    presets[preset_name] = f.read().strip()
            except:
                pass

    return presets

def save_prompt_preset(name: str, prompt: str) -> str:
    """Save a prompt preset to the prompts directory"""
    if not name or not name.strip():
        if current_language == "ru":
            return "âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¸Ğ¼Ñ Ğ¿Ñ€ĞµÑĞµÑ‚Ğ°"
        return "âŒ Please provide a preset name"

    if not prompt or not prompt.strip():
        if current_language == "ru":
            return "âŒ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ"
        return "âŒ Please provide a prompt to save"

    # Sanitize filename
    safe_name = "".join(c for c in name if c.isalnum() or c in "_ -").strip()
    if not safe_name:
        if current_language == "ru":
            return "âŒ ĞĞµĞ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ¿Ñ€ĞµÑĞµÑ‚Ğ°"
        return "âŒ Invalid preset name"

    try:
        os.makedirs(PROMPTS_DIR, exist_ok=True)
        filepath = os.path.join(PROMPTS_DIR, f"{safe_name}.txt")
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(prompt.strip())
        if current_language == "ru":
            return f"âœ… ĞŸÑ€ĞµÑĞµÑ‚ '{safe_name}' ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½!"
        return f"âœ… Preset '{safe_name}' saved successfully!"
    except Exception as e:
        if current_language == "ru":
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: {str(e)}"
        return f"âŒ Error saving preset: {str(e)}"

def save_text_to_file(text: str, filename: str = "result.txt") -> str:
    """Save text to a temporary file and return the path"""
    temp_file = os.path.join(TEMP_DIR, filename)
    with open(temp_file, 'w', encoding='utf-8') as f:
        f.write(text)
    return temp_file

def stop_generation():
    """Set the stop flag to True"""
    global stop_generation_flag
    stop_generation_flag = True
    return "ğŸ›‘ Stopping generation..."

def reset_stop_flag():
    """Reset the stop flag"""
    global stop_generation_flag
    stop_generation_flag = False

# Custom CSS for beautiful UI
CUSTOM_CSS = """
/* Main container styling */
.gradio-container {
    background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%) !important;
}

/* Card style blocks */
.card-style {
    background: rgba(255, 255, 255, 0.9) !important;
    border: 1px solid rgba(203, 213, 225, 0.5) !important;
    box-shadow: 0 8px 32px rgba(100, 116, 139, 0.1) !important;
    border-radius: 16px !important;
    padding: 1.5rem !important;
    margin-bottom: 1rem !important;
}

/* Main header with gradient */
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 20px;
    margin-bottom: 1.5rem;
    text-align: center;
    box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
}

.main-header h1 {
    color: white !important;
    font-size: 2.5rem !important;
    font-weight: 700 !important;
    margin: 0 !important;
    text-shadow: 0 2px 4px rgba(0,0,0,0.3);
}

.main-header p {
    color: rgba(255,255,255,0.9) !important;
    font-size: 1.1rem !important;
    margin: 0.5rem 0 0 0 !important;
}

.main-header a {
    color: white !important;
    text-decoration: underline;
}

/* Generate button styling */
.generate-btn {
    min-height: 36px !important;
    font-size: 0.95rem !important;
    font-weight: 500 !important;
    padding: 8px 16px !important;
}

/* Status box */
.status-box {
    background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%) !important;
    border: 1px solid #7dd3fc !important;
    border-radius: 12px !important;
    padding: 1rem !important;
}

/* Progress info */
.progress-info {
    background: #f0fdf4 !important;
    border: 1px solid #86efac !important;
    border-radius: 8px !important;
    padding: 0.75rem !important;
}

/* Dark mode support */
.dark .gradio-container {
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%) !important;
}

.dark .card-style {
    background: rgba(30, 41, 59, 0.9) !important;
    border: 1px solid rgba(51, 65, 85, 0.5) !important;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3) !important;
}

.dark .status-box {
    background: linear-gradient(135deg, #1e3a5f 0%, #1e293b 100%) !important;
    border: 1px solid #3b82f6 !important;
}

.dark .progress-info {
    background: #14532d !important;
    border: 1px solid #22c55e !important;
}

/* Accordion styling */
.settings-accordion {
    border-radius: 12px !important;
    overflow: hidden !important;
}

/* Result variants */
.variant-box {
    border: 2px solid #e2e8f0;
    border-radius: 12px;
    padding: 1rem;
    margin-bottom: 0.5rem;
    transition: all 0.2s ease;
}

.variant-box:hover {
    border-color: #667eea;
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
}

/* Export buttons */
.export-btn {
    margin: 0.25rem !important;
}
"""

# Description types with prompts
DESCRIPTION_TYPES = {
    "en": {
        "Descriptive (Formal)": "Write a detailed and formal description of this image.",
        "Descriptive (Informal)": "Write a casual, friendly description of this image.",
        "Product Description": "Write a compelling product description for e-commerce based on this image.",
        "SEO Description": "Write an SEO-optimized description for this image, maximum 160 characters.",
        "Stable Diffusion Prompt": "Write a detailed Stable Diffusion prompt to recreate this image.",
        "MidJourney Prompt": "Write a MidJourney prompt to recreate this image.",
        "Booru Tags": "Write a list of Booru-style tags for this image, separated by commas.",
        "Art Critic Analysis": "Analyze this image like an art critic, discussing composition, style, color, lighting, and artistic elements.",
        "Social Media Caption": "Write an engaging social media caption for this image.",
        "OCR: Extract all text": "Extract ALL text from the image. Read every word, number, and symbol visible.",
        "OCR: Text with coordinates": "Extract all text and provide position coordinates [x1, y1, x2, y2] for each text region.",
        "OCR: Table to HTML": "If there is a table, convert it to HTML format using <table>, <tr>, and <td> tags.",
        "OCR: Structured JSON": "Extract all information in structured JSON format with key-value pairs.",
        "ğŸ”€ Compare products": "Compare these product images side by side. List: 1) design differences, 2) color variations, 3) feature changes, 4) quality assessment, 5) which one to recommend and why.",
        "ğŸ”€ Before/After comparison": "Analyze the before (first image) and after (last image) states: 1) What changed? 2) Quantify improvements if measurable, 3) Rate the transformation quality 1-10, 4) What could be improved further?",
        "ğŸ”€ Time-series analysis": "These images show a sequence over time. Describe: 1) progression and trends, 2) identify causality between frames, 3) predict what happens next, 4) rate of change (fast/slow/accelerating), 5) any anomalies.",
        "ğŸ”€ Quality control": "Review these quality control images: 1) identify defects in each, 2) classify defect types, 3) rate pass/fail for each, 4) percentage meeting standards, 5) recommend corrective actions, 6) any systematic issues?",
        "ğŸ“ Detect objects with locations": "Detect all objects in the image and return their locations in the format: {\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"object_name\"}",
        "ğŸ“ Visual grounding": "Describe the image in detail with grounding. For each important object, provide bounding box coordinates.",
        "ğŸ“ Find and locate": "Find all instances of specific objects and provide their precise locations with bounding boxes in JSON format.",
        "ğŸ§  Math step-by-step": "Solve the mathematical problem in the image. Think carefully step by step. Step 1: Identify the data. Step 2: Calculate. Step 3: Verify the answer.",
        "ğŸ§  Logical analysis": "Analyze the image step by step: 1) Identify the main subject, 2) Describe background and context, 3) Note important details, 4) Explain the overall scene and atmosphere.",
        "ğŸ§  Causal analysis": "Examine the sequence and explain: 1) What happened first? 2) What caused the change? 3) What was the effect? 4) What will likely happen next? Provide reasoning for each step.",
        "ğŸ“Š Chart analysis": "You are an expert in visual analytics. Analyze the chart: 1) Chart type, 2) Title and axes, 3) Key trends, 4) Conclusions and insights.",
        "ğŸ“Š Data visualization": "Analyze this data visualization: identify the type, extract data points, describe trends, and provide key insights with specific numbers.",
        "ğŸ©º Medical image analysis": "Analyze this medical image. Identify visible structures and note any abnormalities. Use medical terminology where appropriate.",
        "âš™ï¸ Technical diagram": "This is a technical diagram. Identify all components, explain their functions, and describe how they interact with each other.",
        "ğŸ“„ Document extraction": "Extract structured data from the document without omissions. Return JSON with keys: document_type, date, number, items (array), total.",
        "ğŸ”¬ Scientific image": "Analyze this scientific image. Describe observed phenomena, structures, and processes. Use scientific terminology.",
        "ğŸ¬ Timeline of events": "Describe the chronological sequence of events in the video with timestamps. What happens at each key moment?",
        "ğŸ¬ Action detection": "Identify all instances where [action] occurs in the video and provide approximate timestamps.",
        "ğŸ¬ Long video summary": "This is a [X]-minute video. Summarize the main topics and key points covered.",
        "ğŸ¬ Editing analysis": "Analyze the video editing: shot types, pacing, transitions, background music (if instruments/actions are visible).",
        "ğŸ“š Explain concept": "Explain the concept shown in the image in simple language. How does it work and why is it important?",
        "ğŸ“š Solve textbook problem": "Solve this textbook problem step by step. Show all calculations and explain the logic.",
        "ğŸ“š Historical analysis": "Analyze this historical photo/document: period, context, significance, visible details of the era.",
        "ğŸ“š Lab setup": "Describe the laboratory setup: equipment, procedure, safety measures.",
        "ğŸ¨ Color analysis": "Detailed color palette analysis: dominant colors, contrasts, harmony, color mood, application in design.",
        "ğŸ—ï¸ Architectural analysis": "Architecture analysis: style, period, structural elements, materials, functionality, cultural significance.",
        "ğŸ½ï¸ Dish analysis": "As a chef, describe the dish: ingredients, cooking technique, presentation, flavor combination, serving recommendations.",
        "ğŸ’¼ Presentation slide": "Describe the presentation slide content: title, key points, visual elements, main message.",
        "ğŸ­ Industrial safety": "Analyze the image for safety: identified risks, violations, recommendations.",
        "ğŸ¯ Layered composition": "Analyze the scene in layers: Layer 1: Background, Layer 2: Middle ground, Layer 3: Foreground, Layer 4: Overall composition and visual flow.",
        "ğŸ¯ Spatial analysis": "Describe spatial arrangement: overall layout, foreground/middle/background, object relationships, perspective.",
        "ğŸ¤” Careful analysis": "Before answering, first carefully observe the image. Then, organize your thoughts about what's important. Finally, provide a structured response covering: subject, context, details, and interpretation.",
        "ğŸ¤” Problem-finding": "Examine the image critically. What works well? What could be improved? Provide specific recommendations.",
        "Custom": ""
    },
    "ru": {
        "ĞĞ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ (Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹)": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
        "ĞĞ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ (Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹)": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ½ĞµĞ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´Ñ‘Ğ½Ğ½Ğ¾Ğµ, Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
        "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
        "SEO Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ SEO-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 160 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ².",
        "ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Stable Diffusion": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Stable Diffusion, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.",
        "ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ MidJourney": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ MidJourney, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.",
        "Ğ¢ĞµĞ³Ğ¸ Booru": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚ĞµĞ³Ğ¾Ğ² Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Booru Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ğ¼Ğ¸.",
        "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ²ĞµĞ´Ğ°": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ²ĞµĞ´, Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, ÑÑ‚Ğ¸Ğ»ÑŒ, Ñ†Ğ²ĞµÑ‚, Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
        "ĞŸĞ¾ÑÑ‚ Ğ´Ğ»Ñ ÑĞ¾Ñ†ÑĞµÑ‚ĞµĞ¹": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğº ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
        "OCR: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚": "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ’Ğ•Ğ¡Ğ¬ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾, Ñ†Ğ¸Ñ„Ñ€Ñƒ Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ».",
        "OCR: Ğ¢ĞµĞºÑÑ‚ Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸": "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ: [{'bbox_2d': [x1, y1, x2, y2], 'label': 'Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚'}]. ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ [0, 1000], Ğ³Ğ´Ğµ (0,0) ÑÑ‚Ğ¾ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ¹ Ğ»ĞµĞ²Ñ‹Ğ¹ ÑƒĞ³Ğ¾Ğ», Ğ° (1000,1000) - Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ñ‹Ğ¹.",
        "OCR: Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ² HTML": "Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞ¹ ĞµÑ‘ Ğ² HTML Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ Ñ‚ĞµĞ³Ğ°Ğ¼Ğ¸ <table>, <tr> Ğ¸ <td>.",
        "OCR: Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ JSON": "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ²ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.",
        "ğŸ”€ Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹": "Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸ ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ². ĞŸĞµÑ€ĞµÑ‡Ğ¸ÑĞ»Ğ¸: 1) Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ, 2) Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ², 3) Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, 4) Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, 5) ĞºĞ°ĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑˆÑŒ Ğ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ.",
        "ğŸ”€ Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾/Ğ¿Ğ¾ÑĞ»Ğµ": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ¾ (Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ) Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ): 1) Ğ§Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¾ÑÑŒ? 2) ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, 3) ĞÑ†ĞµĞ½Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 1-10, 4) Ğ§Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ?",
        "ğŸ”€ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑĞ´Ğ°": "Ğ­Ñ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ¿Ğ¸ÑˆĞ¸: 1) Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹, 2) Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, 3) Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ¶Ğ¸ Ñ‡Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ°Ğ»ÑŒÑˆĞµ, 4) ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹, 5) Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸.",
        "ğŸ”€ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: 1) Ğ´ĞµÑ„ĞµĞºÑ‚Ñ‹ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼, 2) Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ², 3) ÑĞ´Ğ°Ñ‡Ğ°/Ğ¾Ñ‚ĞºĞ°Ğ· Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾, 4) Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼, 5) ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, 6) ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹?",
        "ğŸ“ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼": "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ÑŒ Ğ²ÑĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¸ Ğ¸Ñ… Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ: {\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ_Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°\"}",
        "ğŸ“ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ°": "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° ÑƒĞºĞ°Ğ¶Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ€Ğ°Ğ¼ĞºĞ¸.",
        "ğŸ“ ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ": "ĞĞ°Ğ¹Ğ´Ğ¸ Ğ²ÑĞµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ñ‹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞºĞ°Ğ¶Ğ¸ Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸ Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ.",
        "ğŸ§  ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾": "Ğ ĞµÑˆĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑˆĞ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼. Ğ¨Ğ°Ğ³ 1: ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¨Ğ°Ğ³ 2: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸. Ğ¨Ğ°Ğ³ 3: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚.",
        "ğŸ§  Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾: 1) ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚, 2) ĞĞ¿Ğ¸ÑˆĞ¸ Ñ„Ğ¾Ğ½ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, 3) ĞÑ‚Ğ¼ĞµÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, 4) ĞĞ±ÑŠÑÑĞ½Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ†ĞµĞ½Ñƒ Ğ¸ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ñƒ.",
        "ğŸ§  ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "Ğ˜Ğ·ÑƒÑ‡Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸: 1) Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ¾ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°? 2) Ğ§Ñ‚Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ»Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ? 3) ĞšĞ°ĞºĞ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚? 4) Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾Ğ¹Ğ´ĞµÑ‚ Ğ´Ğ°Ğ»ÑŒÑˆĞµ? ĞĞ±Ğ¾ÑĞ½ÑƒĞ¹ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³.",
        "ğŸ“Š ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²": "Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº: 1) Ğ¢Ğ¸Ğ¿ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°, 2) Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¸ Ğ¾ÑĞ¸, 3) ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹, 4) Ğ’Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹.",
        "ğŸ“Š Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ¿Ğ¸ÑˆĞ¸ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ¸ Ğ´Ğ°Ğ¹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ°Ğ¼Ğ¸.",
        "ğŸ©º ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾Ñ‚Ğ¼ĞµÑ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ.",
        "âš™ï¸ Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°": "Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°. ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚.",
        "ğŸ“„ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°": "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ². Ğ’ĞµÑ€Ğ½Ğ¸ JSON Ñ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸: Ñ‚Ğ¸Ğ¿_Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ´Ğ°Ñ‚Ğ°, Ğ½Ğ¾Ğ¼ĞµÑ€, Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ (Ğ¼Ğ°ÑÑĞ¸Ğ²), Ğ¸Ñ‚Ğ¾Ğ³Ğ¾.",
        "ğŸ”¬ ĞĞ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ¿Ğ¸ÑˆĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ.",
        "ğŸ¬ Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑˆĞºĞ°Ğ»Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹": "ĞĞ¿Ğ¸ÑˆĞ¸ Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚?",
        "ğŸ¬ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹": "ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ²ÑĞµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° [Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ] Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ ÑƒĞºĞ°Ğ¶Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸.",
        "ğŸ¬ Ğ ĞµĞ·ÑĞ¼Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾": "Ğ­Ñ‚Ğ¾ [X]-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑĞ¼Ğ¸Ñ€ÑƒĞ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹.",
        "ğŸ¬ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ°": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶ Ğ²Ğ¸Ğ´ĞµĞ¾: Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ², Ñ‚ĞµĞ¼Ğ¿, Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹, Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ (ĞµÑĞ»Ğ¸ Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹/Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ).",
        "ğŸ“š ĞĞ±ÑŠÑÑĞ½Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ": "ĞĞ±ÑŠÑÑĞ½Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼. ĞšĞ°Ğº ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑÑ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾?",
        "ğŸ“š Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ°": "Ğ ĞµÑˆĞ¸ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ· ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾. ĞŸĞ¾ĞºĞ°Ğ¶Ğ¸ Ğ²ÑĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ.",
        "ğŸ“š Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ„Ğ¾Ñ‚Ğ¾/Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚: Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ¿Ğ¾Ñ…Ğ¸.",
        "ğŸ“š Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°": "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ: Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ, Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.",
        "ğŸ¨ Ğ¦Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ»Ğ¸Ñ‚Ñ€Ñ‹: Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ñ‹, Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ.",
        "ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹: ÑÑ‚Ğ¸Ğ»ÑŒ, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´, ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ.",
        "ğŸ½ï¸ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ»ÑĞ´Ğ°": "ĞšĞ°Ğº ÑˆĞµÑ„-Ğ¿Ğ¾Ğ²Ğ°Ñ€ Ğ¾Ğ¿Ğ¸ÑˆĞ¸ Ğ±Ğ»ÑĞ´Ğ¾: Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚Ñ‹, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºÑƒÑĞ¾Ğ², Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğµ.",
        "ğŸ’¼ ĞŸÑ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ/Ğ¡Ğ»Ğ°Ğ¹Ğ´": "ĞĞ¿Ğ¸ÑˆĞ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ°Ğ¹Ğ´Ğ° Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿ÑƒĞ½ĞºÑ‚Ñ‹, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ñ‹ÑĞ»ÑŒ.",
        "ğŸ­ ĞŸÑ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ": "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸, Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸.",
        "ğŸ¯ ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼": "ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ†ĞµĞ½Ñƒ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾: Ğ¡Ğ»Ğ¾Ğ¹ 1: Ğ¤Ğ¾Ğ½, Ğ¡Ğ»Ğ¾Ğ¹ 2: Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ¡Ğ»Ğ¾Ğ¹ 3: ĞŸĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ¡Ğ»Ğ¾Ğ¹ 4: ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ°.",
        "ğŸ¯ ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±Ñ‰Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ°, Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹/ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹/Ğ·Ğ°Ğ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°.",
        "ğŸ¤” Ğ’Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·": "ĞŸĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞ¹ ÑĞ²Ğ¾Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¼. ĞĞ°ĞºĞ¾Ğ½ĞµÑ†, Ğ´Ğ°Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚: Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ.",
        "ğŸ¤” ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼": "Ğ˜Ğ·ÑƒÑ‡Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸. Ğ§Ñ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾? Ğ§Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ? ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸.",
        "Ğ¡Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚": ""
    },
    "zh": {
        "æè¿°æ€§ï¼ˆæ­£å¼ï¼‰": "å†™ä¸€ä¸ªè¯¦ç»†æ­£å¼çš„å›¾åƒæè¿°ã€‚",
        "æè¿°æ€§ï¼ˆéæ­£å¼ï¼‰": "å†™ä¸€ä¸ªè½»æ¾å‹å¥½çš„å›¾åƒæè¿°ã€‚",
        "äº§å“æè¿°": "æ ¹æ®è¿™å¼ å›¾ç‰‡ä¸ºç”µå•†å¹³å°å†™ä¸€ä¸ªå¸å¼•äººçš„äº§å“æè¿°ã€‚",
        "SEOæè¿°": "ä¸ºè¿™å¼ å›¾ç‰‡å†™ä¸€ä¸ªSEOä¼˜åŒ–çš„æè¿°ï¼Œæœ€å¤š160ä¸ªå­—ç¬¦ã€‚",
        "Stable Diffusionæç¤ºè¯": "å†™ä¸€ä¸ªè¯¦ç»†çš„Stable Diffusionæç¤ºè¯æ¥é‡ç°è¿™å¼ å›¾ç‰‡ã€‚",
        "MidJourneyæç¤ºè¯": "å†™ä¸€ä¸ªMidJourneyæç¤ºè¯æ¥é‡ç°è¿™å¼ å›¾ç‰‡ã€‚",
        "Booruæ ‡ç­¾": "ä¸ºè¿™å¼ å›¾ç‰‡å†™ä¸€ä¸ªBoorué£æ ¼çš„æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚",
        "è‰ºæœ¯è¯„è®ºåˆ†æ": "åƒè‰ºæœ¯è¯„è®ºå®¶ä¸€æ ·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œè®¨è®ºæ„å›¾ã€é£æ ¼ã€è‰²å½©ã€å…‰çº¿å’Œè‰ºæœ¯å…ƒç´ ã€‚",
        "ç¤¾äº¤åª’ä½“æ–‡æ¡ˆ": "ä¸ºè¿™å¼ å›¾ç‰‡å†™ä¸€ä¸ªå¸å¼•äººçš„ç¤¾äº¤åª’ä½“æ–‡æ¡ˆã€‚",
        "OCR: æå–æ‰€æœ‰æ–‡å­—": "æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚è¯»å–æ¯ä¸ªå•è¯ã€æ•°å­—å’Œç¬¦å·ã€‚",
        "OCR: æ–‡å­—ä¸åæ ‡": "æå–æ‰€æœ‰æ–‡å­—å¹¶æä¾›æ¯ä¸ªæ–‡å­—åŒºåŸŸçš„ä½ç½®åæ ‡[x1, y1, x2, y2]ã€‚",
        "OCR: è¡¨æ ¼è½¬HTML": "å¦‚æœæœ‰è¡¨æ ¼ï¼Œå°†å…¶è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œä½¿ç”¨<table>ã€<tr>å’Œ<td>æ ‡ç­¾ã€‚",
        "OCR: ç»“æ„åŒ–JSON": "ä»¥ç»“æ„åŒ–JSONæ ¼å¼æå–æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…å«é”®å€¼å¯¹ã€‚",
        "ğŸ”€ æ¯”è¾ƒäº§å“": "å¹¶æ’æ¯”è¾ƒè¿™äº›äº§å“å›¾åƒã€‚åˆ—å‡ºï¼š1ï¼‰è®¾è®¡å·®å¼‚ï¼Œ2ï¼‰é¢œè‰²å˜åŒ–ï¼Œ3ï¼‰åŠŸèƒ½å˜åŒ–ï¼Œ4ï¼‰è´¨é‡è¯„ä¼°ï¼Œ5ï¼‰æ¨èå“ªä¸€ä¸ªåŠåŸå› ã€‚",
        "ğŸ”€ å‰åå¯¹æ¯”": "åˆ†æä¹‹å‰ï¼ˆç¬¬ä¸€å¼ å›¾ï¼‰å’Œä¹‹åï¼ˆæœ€åä¸€å¼ ï¼‰çš„çŠ¶æ€ï¼š1ï¼‰å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿ2ï¼‰é‡åŒ–æ”¹è¿›ï¼ˆå¦‚å¯è¡¡é‡ï¼‰ï¼Œ3ï¼‰è½¬æ¢è´¨é‡è¯„åˆ†1-10ï¼Œ4ï¼‰è¿˜å¯ä»¥æ”¹è¿›ä»€ä¹ˆï¼Ÿ",
        "ğŸ”€ æ—¶é—´åºåˆ—åˆ†æ": "è¿™äº›å›¾åƒæ˜¾ç¤ºæ—¶é—´åºåˆ—ã€‚æè¿°ï¼š1ï¼‰è¿›åº¦å’Œè¶‹åŠ¿ï¼Œ2ï¼‰è¯†åˆ«å¸§ä¹‹é—´çš„å› æœå…³ç³»ï¼Œ3ï¼‰é¢„æµ‹æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œ4ï¼‰å˜åŒ–é€Ÿç‡ï¼ˆå¿«/æ…¢/åŠ é€Ÿï¼‰ï¼Œ5ï¼‰ä»»ä½•å¼‚å¸¸ã€‚",
        "ğŸ”€ è´¨é‡æ§åˆ¶": "å®¡æŸ¥è¿™äº›è´¨é‡æ§åˆ¶å›¾åƒï¼š1ï¼‰è¯†åˆ«æ¯ä¸ªç¼ºé™·ï¼Œ2ï¼‰åˆ†ç±»ç¼ºé™·ç±»å‹ï¼Œ3ï¼‰å¯¹æ¯ä¸ªè¯„åˆ†åˆæ ¼/ä¸åˆæ ¼ï¼Œ4ï¼‰ç¬¦åˆæ ‡å‡†çš„ç™¾åˆ†æ¯”ï¼Œ5ï¼‰æ¨èçº æ­£æªæ–½ï¼Œ6ï¼‰æ˜¯å¦æœ‰ç³»ç»Ÿæ€§é—®é¢˜ï¼Ÿ",
        "ğŸ“ æ£€æµ‹å¯¹è±¡åŠä½ç½®": "æ£€æµ‹å›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡å¹¶ä»¥æ ¼å¼è¿”å›å…¶ä½ç½®ï¼š{\"bbox_2d\": [x1, y1, x2, y2], \"label\": \"å¯¹è±¡åç§°\"}",
        "ğŸ“ è§†è§‰å®šä½": "è¯¦ç»†æè¿°å›¾åƒå¹¶æä¾›å®šä½ã€‚å¯¹äºæ¯ä¸ªé‡è¦å¯¹è±¡ï¼Œæä¾›è¾¹ç•Œæ¡†åæ ‡ã€‚",
        "ğŸ“ æŸ¥æ‰¾å¹¶å®šä½": "æŸ¥æ‰¾ç‰¹å®šå¯¹è±¡çš„æ‰€æœ‰å®ä¾‹ï¼Œå¹¶ä»¥JSONæ ¼å¼æä¾›å…¶ç²¾ç¡®ä½ç½®å’Œè¾¹ç•Œæ¡†ã€‚",
        "ğŸ§  é€æ­¥æ•°å­¦": "è§£å†³å›¾åƒä¸­çš„æ•°å­¦é—®é¢˜ã€‚é€æ­¥ä»”ç»†æ€è€ƒã€‚æ­¥éª¤1ï¼šè¯†åˆ«æ•°æ®ã€‚æ­¥éª¤2ï¼šè®¡ç®—ã€‚æ­¥éª¤3ï¼šéªŒè¯ç­”æ¡ˆã€‚",
        "ğŸ§  é€»è¾‘åˆ†æ": "é€æ­¥åˆ†æå›¾åƒï¼š1ï¼‰è¯†åˆ«ä¸»è¦å¯¹è±¡ï¼Œ2ï¼‰æè¿°èƒŒæ™¯å’Œä¸Šä¸‹æ–‡ï¼Œ3ï¼‰æ³¨æ„é‡è¦ç»†èŠ‚ï¼Œ4ï¼‰è§£é‡Šæ•´ä½“åœºæ™¯å’Œæ°›å›´ã€‚",
        "ğŸ§  å› æœåˆ†æ": "æ£€æŸ¥åºåˆ—å¹¶è§£é‡Šï¼š1ï¼‰é¦–å…ˆå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ2ï¼‰ä»€ä¹ˆå¯¼è‡´äº†å˜åŒ–ï¼Ÿ3ï¼‰æ•ˆæœæ˜¯ä»€ä¹ˆï¼Ÿ4ï¼‰æ¥ä¸‹æ¥å¯èƒ½å‘ç”Ÿä»€ä¹ˆï¼Ÿä¸ºæ¯ä¸ªæ­¥éª¤æä¾›æ¨ç†ã€‚",
        "ğŸ“Š å›¾è¡¨åˆ†æ": "ä½ æ˜¯è§†è§‰åˆ†æä¸“å®¶ã€‚åˆ†æå›¾è¡¨ï¼š1ï¼‰å›¾è¡¨ç±»å‹ï¼Œ2ï¼‰æ ‡é¢˜å’Œåæ ‡è½´ï¼Œ3ï¼‰å…³é”®è¶‹åŠ¿ï¼Œ4ï¼‰ç»“è®ºå’Œè§è§£ã€‚",
        "ğŸ“Š æ•°æ®å¯è§†åŒ–": "åˆ†æè¿™ä¸ªæ•°æ®å¯è§†åŒ–ï¼šè¯†åˆ«ç±»å‹ï¼Œæå–æ•°æ®ç‚¹ï¼Œæè¿°è¶‹åŠ¿ï¼Œå¹¶æä¾›å…·ä½“æ•°å­—çš„å…³é”®è§è§£ã€‚",
        "ğŸ©º åŒ»å­¦å›¾åƒåˆ†æ": "åˆ†æè¿™å¼ åŒ»å­¦å›¾åƒã€‚è¯†åˆ«å¯è§ç»“æ„å¹¶æ³¨æ„ä»»ä½•å¼‚å¸¸ã€‚é€‚å½“ä½¿ç”¨åŒ»å­¦æœ¯è¯­ã€‚",
        "âš™ï¸ æŠ€æœ¯å›¾è¡¨": "è¿™æ˜¯æŠ€æœ¯å›¾è¡¨ã€‚è¯†åˆ«æ‰€æœ‰ç»„ä»¶ï¼Œè§£é‡Šå…¶åŠŸèƒ½ï¼Œå¹¶æè¿°å®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚",
        "ğŸ“„ æ–‡æ¡£æå–": "ä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–æ•°æ®è€Œä¸é—æ¼ã€‚è¿”å›JSONï¼Œé”®ï¼šdocument_typeã€dateã€numberã€itemsï¼ˆæ•°ç»„ï¼‰ã€totalã€‚",
        "ğŸ”¬ ç§‘å­¦å›¾åƒ": "åˆ†æè¿™å¼ ç§‘å­¦å›¾åƒã€‚æè¿°è§‚å¯Ÿåˆ°çš„ç°è±¡ã€ç»“æ„å’Œè¿‡ç¨‹ã€‚ä½¿ç”¨ç§‘å­¦æœ¯è¯­ã€‚",
        "ğŸ¬ äº‹ä»¶æ—¶é—´çº¿": "æè¿°è§†é¢‘ä¸­äº‹ä»¶çš„æ—¶é—´é¡ºåºå’Œæ—¶é—´æˆ³ã€‚æ¯ä¸ªå…³é”®æ—¶åˆ»å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ",
        "ğŸ¬ åŠ¨ä½œæ£€æµ‹": "è¯†åˆ«è§†é¢‘ä¸­å‘ç”Ÿ[åŠ¨ä½œ]çš„æ‰€æœ‰å®ä¾‹ï¼Œå¹¶æä¾›å¤§è‡´æ—¶é—´æˆ³ã€‚",
        "ğŸ¬ é•¿è§†é¢‘æ‘˜è¦": "è¿™æ˜¯ä¸€ä¸ª[X]åˆ†é’Ÿçš„è§†é¢‘ã€‚æ€»ç»“æ¶µç›–çš„ä¸»è¦ä¸»é¢˜å’Œè¦ç‚¹ã€‚",
        "ğŸ¬ å‰ªè¾‘åˆ†æ": "åˆ†æè§†é¢‘å‰ªè¾‘ï¼šé•œå¤´ç±»å‹ã€èŠ‚å¥ã€è½¬åœºã€èƒŒæ™¯éŸ³ä¹ï¼ˆå¦‚æœå¯è§ä¹å™¨/åŠ¨ä½œï¼‰ã€‚",
        "ğŸ“š è§£é‡Šæ¦‚å¿µ": "ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šå›¾åƒä¸­æ˜¾ç¤ºçš„æ¦‚å¿µã€‚å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ",
        "ğŸ“š è§£å†³æ•™ç§‘ä¹¦é—®é¢˜": "é€æ­¥è§£å†³è¿™ä¸ªæ•™ç§‘ä¹¦é—®é¢˜ã€‚æ˜¾ç¤ºæ‰€æœ‰è®¡ç®—å¹¶è§£é‡Šé€»è¾‘ã€‚",
        "ğŸ“š å†å²åˆ†æ": "åˆ†æè¿™å¼ å†å²ç…§ç‰‡/æ–‡ä»¶ï¼šæ—¶æœŸã€èƒŒæ™¯ã€æ„ä¹‰ã€æ—¶ä»£çš„å¯è§ç»†èŠ‚ã€‚",
        "ğŸ“š å®éªŒå®¤è®¾ç½®": "æè¿°å®éªŒå®¤è®¾ç½®ï¼šè®¾å¤‡ã€ç¨‹åºã€å®‰å…¨æªæ–½ã€‚",
        "ğŸ¨ è‰²å½©åˆ†æ": "è¯¦ç»†çš„è‰²å½©è°ƒè‰²æ¿åˆ†æï¼šä¸»è‰²ã€å¯¹æ¯”ã€å’Œè°ã€è‰²å½©æƒ…ç»ªã€è®¾è®¡åº”ç”¨ã€‚",
        "ğŸ—ï¸ å»ºç­‘åˆ†æ": "å»ºç­‘åˆ†æï¼šé£æ ¼ã€æ—¶æœŸã€ç»“æ„å…ƒç´ ã€ææ–™ã€åŠŸèƒ½ã€æ–‡åŒ–æ„ä¹‰ã€‚",
        "ğŸ½ï¸ èœè‚´åˆ†æ": "ä½œä¸ºå¨å¸ˆï¼Œæè¿°èœè‚´ï¼šé…æ–™ã€çƒ¹é¥ªæŠ€æœ¯ã€æ‘†ç›˜ã€é£å‘³ç»„åˆã€ä¸Šèœå»ºè®®ã€‚",
        "ğŸ’¼ æ¼”ç¤ºå¹»ç¯ç‰‡": "æè¿°æ¼”ç¤ºå¹»ç¯ç‰‡å†…å®¹ï¼šæ ‡é¢˜ã€è¦ç‚¹ã€è§†è§‰å…ƒç´ ã€ä¸»è¦ä¿¡æ¯ã€‚",
        "ğŸ­ å·¥ä¸šå®‰å…¨": "åˆ†æå›¾åƒçš„å®‰å…¨æ€§ï¼šè¯†åˆ«çš„é£é™©ã€è¿è§„ã€å»ºè®®ã€‚",
        "ğŸ¯ åˆ†å±‚æ„å›¾": "åˆ†å±‚åˆ†æåœºæ™¯ï¼šç¬¬1å±‚ï¼šèƒŒæ™¯ï¼Œç¬¬2å±‚ï¼šä¸­æ™¯ï¼Œç¬¬3å±‚ï¼šå‰æ™¯ï¼Œç¬¬4å±‚ï¼šæ•´ä½“æ„å›¾å’Œè§†è§‰æµã€‚",
        "ğŸ¯ ç©ºé—´åˆ†æ": "æè¿°ç©ºé—´å¸ƒå±€ï¼šæ•´ä½“å¸ƒå±€ã€å‰æ™¯/ä¸­æ™¯/èƒŒæ™¯ã€å¯¹è±¡å…³ç³»ã€é€è§†ã€‚",
        "ğŸ¤” ä»”ç»†åˆ†æ": "åœ¨å›ç­”ä¹‹å‰ï¼Œé¦–å…ˆä»”ç»†è§‚å¯Ÿå›¾åƒã€‚ç„¶åï¼Œæ•´ç†å…³äºé‡è¦å†…å®¹çš„æƒ³æ³•ã€‚æœ€åï¼Œæä¾›æ¶µç›–ä»¥ä¸‹å†…å®¹çš„ç»“æ„åŒ–å“åº”ï¼šä¸»é¢˜ã€ä¸Šä¸‹æ–‡ã€ç»†èŠ‚å’Œè§£é‡Šã€‚",
        "ğŸ¤” é—®é¢˜å‘ç°": "æ‰¹åˆ¤æ€§åœ°æ£€æŸ¥å›¾åƒã€‚ä»€ä¹ˆæ•ˆæœå¥½ï¼Ÿä»€ä¹ˆå¯ä»¥æ”¹è¿›ï¼Ÿæä¾›å…·ä½“å»ºè®®ã€‚",
        "è‡ªå®šä¹‰": ""
    }
}

# Description lengths
DESCRIPTION_LENGTHS = {
    "en": {
        "Any": "",
        "Very Short (1-2 sentences)": "Keep it very short, 1-2 sentences maximum.",
        "Short (3-4 sentences)": "Keep it short, 3-4 sentences.",
        "Medium (1 paragraph)": "Write a medium-length description, about one paragraph.",
        "Long (2-3 paragraphs)": "Write a detailed description, 2-3 paragraphs.",
        "Very Long (comprehensive)": "Write a comprehensive and very detailed description."
    },
    "ru": {
        "Ğ›ÑĞ±Ğ°Ñ": "",
        "ĞÑ‡ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)": "Ğ¡Ğ´ĞµĞ»Ğ°Ğ¹ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾, Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.",
        "ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ (3-4 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)": "Ğ¡Ğ´ĞµĞ»Ğ°Ğ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾, 3-4 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.",
        "Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ (1 Ğ°Ğ±Ğ·Ğ°Ñ†)": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¾ĞºĞ¾Ğ»Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°.",
        "Ğ”Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ (2-3 Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°)": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, 2-3 Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°.",
        "ĞÑ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ (Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ°Ñ)": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ."
    },
    "zh": {
        "ä»»æ„": "",
        "éå¸¸çŸ­ï¼ˆ1-2å¥ï¼‰": "ä¿æŒéå¸¸ç®€çŸ­ï¼Œæœ€å¤š1-2å¥ã€‚",
        "çŸ­ï¼ˆ3-4å¥ï¼‰": "ä¿æŒç®€çŸ­ï¼Œ3-4å¥ã€‚",
        "ä¸­ç­‰ï¼ˆ1æ®µï¼‰": "å†™ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æè¿°ï¼Œå¤§çº¦ä¸€æ®µã€‚",
        "é•¿ï¼ˆ2-3æ®µï¼‰": "å†™ä¸€ä¸ªè¯¦ç»†çš„æè¿°ï¼Œ2-3æ®µã€‚",
        "éå¸¸é•¿ï¼ˆå…¨é¢ï¼‰": "å†™ä¸€ä¸ªå…¨é¢ä¸”éå¸¸è¯¦ç»†çš„æè¿°ã€‚"
    }
}

# Multi-language support
TRANSLATIONS = {
    "en": {
        "title": "Qwen VL Image Description Generator",
        "header": "ğŸ–¼ï¸ Image Description Generator based on Qwen Vision Language Models",
        "subtitle": "Upload an image and enter a prompt to generate a description using Qwen VL models.",
        "language": "Language",
        "language_info": "Select language",
        "model_selection": "Model Selection",
        "model_info": "Select a model for generating descriptions",
        "advanced_params": "âš™ï¸ Advanced Parameters",
        "max_tokens": "Max New Tokens",
        "max_tokens_info": "Maximum number of tokens to generate",
        "temperature": "Temperature",
        "temperature_info": "Controls randomness of generation",
        "top_p": "Top-p (nucleus sampling)",
        "top_p_info": "Probability threshold for token sampling",
        "top_k": "Top-k",
        "top_k_info": "Number of most probable tokens to consider",
        "seed": "Seed",
        "seed_info": "Seed for reproducibility (-1 for random)",
        "random_seed_btn": "ğŸ² Random Seed",
        "single_processing": "ğŸ“„ Single Processing",
        "batch_processing": "ğŸ“š Batch Processing",
        "upload_image": "Upload Image",
        "image_url": "Or enter Image URL",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "Prompt",
        "prompt_placeholder": "For example: Create a product description for online store",
        "generate_btn": "ğŸš€ Generate Description",
        "result": "Result",
        "upload_images": "Upload Images",
        "prompts_multiline": "Prompts (one per line)",
        "prompts_placeholder": "Create a product description for online store\nCreate SEO Description for product\n...",
        "prompts_info": "Specify one prompt for all images or one prompt per image",
        "process_batch_btn": "ğŸš€ Process Batch",
        "results": "Results",
        "examples_title": "ğŸ’¡ Example Prompts:",
        "example_1": "Create a product description for online store",
        "example_2": "Create an SEO description for a product with a maximum of 160 characters.",
        "example_3": "Create an attractive product description for marketplace",
        "example_4": "Describe image in detail for product catalog",
        "error_no_image": "Please upload an image or provide an image URL",
        "error_no_prompt": "Please enter a prompt",
        "error_no_images": "Please upload images",
        "error_no_prompts": "Please enter prompts (one per line)",
        "error_prompt_mismatch": "Number of prompts ({}) does not match number of images ({}). Specify either one prompt for all images or one prompt per image.",
        "error_generation": "Error generating description: {}",
        "error_url_load": "Error loading image from URL: {}",
        "loading_model": "Loading model: {}",
        "model_loaded": "Model {} successfully loaded on {}",
        "image_label": "=== Image {}: {} ===",
        "prompt_label": "Prompt: {}",
        "result_label": "Result: {}",
        "model_size_warning": "âš ï¸ Note: Large models (8B+) may use CPU offloading if GPU memory is insufficient, which can slow down generation.",
        "quantization": "Quantization",
        "quantization_info": "Memory optimization (4-bit = ~75% less VRAM)",
        "quant_4bit": "4-bit (Recommended)",
        "quant_8bit": "8-bit (Better quality)",
        "quant_none": "None (Full precision)",
        "description_type": "Description Type",
        "description_type_info": "Select the type/style of description",
        "description_length": "Description Length",
        "description_length_info": "Select desired length",
        "num_variants": "Number of Variants",
        "num_variants_info": "Generate multiple description variants",
        "custom_prompt_override": "Custom Prompt (overrides type selection)",
        "custom_prompt_placeholder": "Enter your custom prompt here...",
        "status": "Status",
        "processing_time": "Processing time",
        "generating": "â³ Generating...",
        "stop_btn": "ğŸ›‘ Stop",
        "save_results": "ğŸ’¾ Save Results",
        "output_folder": "Output Folder Name",
        "output_folder_placeholder": "my_dataset",
        "export_format": "Export Format",
        "export_txt": "TXT (one file per image)",
        "export_json": "JSON (all results)",
        "export_csv": "CSV (table format)",
        "variant": "Variant",
        "copy_btn": "ğŸ“‹ Copy",
        "download_btn": "â¬‡ï¸ Download",
        "generation_complete": "âœ… Generation complete!",
        "seconds": "seconds",
        "processing_image": "Processing image",
        "of": "of",
        "extra_options": "Extra Options",
        "extra_options_info": "Additional modifiers for description",
        "character_name": "Character/Person Name",
        "character_name_placeholder": "e.g., John, Alice, or leave empty",
        "character_name_info": "If provided, will use this name instead of generic terms",
        "prompt_preset": "Prompt Preset",
        "prompt_preset_info": "Load a preset from prompts/ folder",
        "refresh_presets": "Refresh",
        "memory_usage": "Memory Usage",
        "download_result": "Download Result",
        "generation_stopped": "Generation stopped by user",
        "stopping": "Stopping..."
    },
    "ru": {
        "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Qwen VL",
        "header": "ğŸ–¼ï¸ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen Vision Language Models",
        "subtitle": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen VL.",
        "language": "Ğ¯Ğ·Ñ‹Ğº",
        "language_info": "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ÑĞ·Ñ‹Ğº",
        "model_selection": "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
        "model_info": "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹",
        "advanced_params": "âš™ï¸ Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹",
        "max_tokens": "ĞœĞ°ĞºÑ. ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
        "max_tokens_info": "ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
        "temperature": "Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°",
        "temperature_info": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
        "top_p": "Top-p (nucleus sampling)",
        "top_p_info": "Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
        "top_k": "Top-k",
        "top_k_info": "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ¸Ñ",
        "seed": "Seed",
        "seed_info": "Seed Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (-1 Ğ´Ğ»Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾)",
        "random_seed_btn": "ğŸ² Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ seed",
        "single_processing": "ğŸ“„ ĞĞ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°",
        "batch_processing": "ğŸ“š ĞŸĞ°ĞºĞµÑ‚Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°",
        "upload_image": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
        "image_url": "Ğ˜Ğ»Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ URL Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "ĞŸÑ€Ğ¾Ğ¼Ñ‚",
        "prompt_placeholder": "ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ°",
        "generate_btn": "ğŸš€ Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ",
        "result": "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
        "upload_images": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
        "prompts_multiline": "ĞŸÑ€Ğ¾Ğ¼Ñ‚Ñ‹ (Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºÑƒ)",
        "prompts_placeholder": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ°\nĞ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ SEO-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°\n...",
        "prompts_info": "Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ñƒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
        "process_batch_btn": "ğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ¿Ğ°ĞºĞµÑ‚",
        "results": "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹",
        "examples_title": "ğŸ’¡ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ğ¾Ğ²:",
        "example_1": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° ''  Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
        "example_2": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ SEO-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 160 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
        "example_3": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹ÑĞ° Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
        "example_4": "Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
        "error_no_image": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ ÑƒĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ URL Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
        "error_no_prompt": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚",
        "error_no_images": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
        "error_no_prompts": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ñ‹ (Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºÑƒ)",
        "error_prompt_mismatch": "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ğ¾Ğ² ({}) Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ({}). Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»Ğ¸Ğ±Ğ¾ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ñƒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.",
        "error_generation": "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ: {}",
        "error_url_load": "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ URL: {}",
        "loading_model": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {}",
        "model_loaded": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ {} ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ½Ğ° {}",
        "image_label": "=== Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ {}: {} ===",
        "prompt_label": "ĞŸÑ€Ğ¾Ğ¼Ñ‚: {}",
        "result_label": "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: {}",
        "model_size_warning": "âš ï¸ ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (8B+) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° CPU Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ.",
        "quantization": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
        "quantization_info": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (4-bit = ~75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)",
        "quant_4bit": "4-bit (Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)",
        "quant_8bit": "8-bit (Ğ›ÑƒÑ‡ÑˆĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾)",
        "quant_none": "ĞĞµÑ‚ (ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ)",
        "description_type": "Ğ¢Ğ¸Ğ¿ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
        "description_type_info": "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ñ‚Ğ¸Ğ¿/ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
        "description_length": "Ğ”Ğ»Ğ¸Ğ½Ğ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
        "description_length_info": "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¶ĞµĞ»Ğ°ĞµĞ¼ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ",
        "num_variants": "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²",
        "num_variants_info": "Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
        "custom_prompt_override": "Ğ¡Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ (Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¸Ğ¿Ğ°)",
        "custom_prompt_placeholder": "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ·Ğ´ĞµÑÑŒ...",
        "status": "Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ",
        "processing_time": "Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
        "generating": "â³ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ...",
        "stop_btn": "ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ",
        "save_results": "ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹",
        "output_folder": "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¿ĞºĞ¸",
        "output_folder_placeholder": "Ğ¼Ğ¾Ğ¹_Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚",
        "export_format": "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°",
        "export_txt": "TXT (Ğ¾Ğ´Ğ¸Ğ½ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ)",
        "export_json": "JSON (Ğ²ÑĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹)",
        "export_csv": "CSV (Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚)",
        "variant": "Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚",
        "copy_btn": "ğŸ“‹ ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
        "download_btn": "â¬‡ï¸ Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ",
        "generation_complete": "âœ… Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!",
        "seconds": "ÑĞµĞºÑƒĞ½Ğ´",
        "processing_image": "ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
        "of": "Ğ¸Ğ·",
        "extra_options": "Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸",
        "extra_options_info": "Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
        "character_name": "Ğ˜Ğ¼Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°/Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
        "character_name_placeholder": "Ğ½Ğ°Ğ¿Ñ€., Ğ˜Ğ²Ğ°Ğ½, ĞĞ»Ğ¸ÑĞ°, Ğ¸Ğ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼",
        "character_name_info": "Ğ•ÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²",
        "prompt_preset": "ĞŸÑ€ĞµÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°",
        "prompt_preset_info": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµÑĞµÑ‚ Ğ¸Ğ· Ğ¿Ğ°Ğ¿ĞºĞ¸ prompts/",
        "refresh_presets": "ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ",
        "memory_usage": "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
        "download_result": "Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
        "generation_stopped": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼",
        "stopping": "ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°..."
    },
    "zh": {
        "title": "Qwen VL å›¾åƒæè¿°ç”Ÿæˆå™¨",
        "header": "ğŸ–¼ï¸ åŸºäº Qwen Vision Language Models çš„å›¾åƒæè¿°ç”Ÿæˆå™¨",
        "subtitle": "ä¸Šä¼ å›¾åƒå¹¶è¾“å…¥æç¤ºè¯ï¼Œä½¿ç”¨ Qwen VL æ¨¡å‹ç”Ÿæˆæè¿°ã€‚",
        "language": "è¯­è¨€",
        "language_info": "é€‰æ‹©è¯­è¨€",
        "model_selection": "æ¨¡å‹é€‰æ‹©",
        "model_info": "é€‰æ‹©ç”¨äºç”Ÿæˆæè¿°çš„æ¨¡å‹",
        "advanced_params": "âš™ï¸ é«˜çº§å‚æ•°",
        "max_tokens": "æœ€å¤§æ–°ä»¤ç‰Œæ•°",
        "max_tokens_info": "ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°",
        "temperature": "æ¸©åº¦",
        "temperature_info": "æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§",
        "top_p": "Top-pï¼ˆæ ¸é‡‡æ ·ï¼‰",
        "top_p_info": "ä»¤ç‰Œé‡‡æ ·çš„æ¦‚ç‡é˜ˆå€¼",
        "top_k": "Top-k",
        "top_k_info": "è€ƒè™‘çš„æœ€å¯èƒ½ä»¤ç‰Œæ•°",
        "seed": "éšæœºç§å­",
        "seed_info": "ç”¨äºå¯é‡ç°æ€§çš„ç§å­ï¼ˆ-1 è¡¨ç¤ºéšæœºï¼‰",
        "random_seed_btn": "ğŸ² éšæœºç§å­",
        "single_processing": "ğŸ“„ å•å¼ å¤„ç†",
        "batch_processing": "ğŸ“š æ‰¹é‡å¤„ç†",
        "upload_image": "ä¸Šä¼ å›¾åƒ",
        "image_url": "æˆ–è¾“å…¥å›¾åƒURL",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "æç¤ºè¯",
        "prompt_placeholder": "ä¾‹å¦‚ï¼šä¸ºåœ¨çº¿å•†åº—åˆ›å»ºäº§å“æè¿°",
        "generate_btn": "ğŸš€ ç”Ÿæˆæè¿°",
        "result": "ç»“æœ",
        "upload_images": "ä¸Šä¼ å›¾åƒ",
        "prompts_multiline": "æç¤ºè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
        "prompts_placeholder": "ä¸ºåœ¨çº¿å•†åº—åˆ›å»ºäº§å“æè¿°\nä¸ºäº§å“åˆ›å»ºSEOæè¿°\n...",
        "prompts_info": "ä¸ºæ‰€æœ‰å›¾åƒæŒ‡å®šä¸€ä¸ªæç¤ºè¯ï¼Œæˆ–ä¸ºæ¯ä¸ªå›¾åƒæŒ‡å®šä¸€ä¸ªæç¤ºè¯",
        "process_batch_btn": "ğŸš€ å¤„ç†æ‰¹æ¬¡",
        "results": "ç»“æœ",
        "examples_title": "ğŸ’¡ ç¤ºä¾‹æç¤ºè¯ï¼š",
        "example_1": "ä¸ºåœ¨çº¿å•†åº—åˆ›å»ºäº§å“æè¿°",
        "example_2": "ä¸ºäº§å“åˆ›å»ºSEOæè¿°æœ€å¤š 160 ä¸ªå­—ç¬¦",
        "example_3": "ä¸ºå¸‚åœºåˆ›å»ºæœ‰å¸å¼•åŠ›çš„äº§å“æè¿°",
        "example_4": "è¯¦ç»†æè¿°äº§å“ç›®å½•çš„å›¾åƒ",
        "error_no_image": "è¯·ä¸Šä¼ å›¾åƒæˆ–æä¾›å›¾åƒURL",
        "error_no_prompt": "è¯·è¾“å…¥æç¤ºè¯",
        "error_no_images": "è¯·ä¸Šä¼ å›¾åƒ",
        "error_no_prompts": "è¯·è¾“å…¥æç¤ºè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
        "error_prompt_mismatch": "æç¤ºè¯æ•°é‡ï¼ˆ{}ï¼‰ä¸å›¾åƒæ•°é‡ï¼ˆ{}ï¼‰ä¸åŒ¹é…ã€‚è¯·ä¸ºæ‰€æœ‰å›¾åƒæŒ‡å®šä¸€ä¸ªæç¤ºè¯ï¼Œæˆ–ä¸ºæ¯ä¸ªå›¾åƒæŒ‡å®šä¸€ä¸ªæç¤ºè¯ã€‚",
        "error_generation": "ç”Ÿæˆæè¿°æ—¶å‡ºé”™ï¼š{}",
        "error_url_load": "ä»URLåŠ è½½å›¾åƒæ—¶å‡ºé”™ï¼š{}",
        "loading_model": "æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{}",
        "model_loaded": "æ¨¡å‹ {} å·²æˆåŠŸåŠ è½½åˆ° {}",
        "image_label": "=== å›¾åƒ {}: {} ===",
        "prompt_label": "æç¤ºè¯ï¼š{}",
        "result_label": "ç»“æœï¼š{}",
        "model_size_warning": "âš ï¸ æ³¨æ„ï¼šå¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œå¤§å‹æ¨¡å‹ï¼ˆ8B+ï¼‰å¯èƒ½ä¼šä½¿ç”¨ CPU å¸è½½ï¼Œè¿™å¯èƒ½ä¼šå‡æ…¢ç”Ÿæˆé€Ÿåº¦ã€‚",
        "quantization": "é‡åŒ–",
        "quantization_info": "å†…å­˜ä¼˜åŒ–ï¼ˆ4ä½ = å‡å°‘çº¦75%æ˜¾å­˜ï¼‰",
        "quant_4bit": "4ä½ï¼ˆæ¨èï¼‰",
        "quant_8bit": "8ä½ï¼ˆæ›´é«˜è´¨é‡ï¼‰",
        "quant_none": "æ— ï¼ˆå…¨ç²¾åº¦ï¼‰",
        "description_type": "æè¿°ç±»å‹",
        "description_type_info": "é€‰æ‹©æè¿°ç±»å‹/é£æ ¼",
        "description_length": "æè¿°é•¿åº¦",
        "description_length_info": "é€‰æ‹©æ‰€éœ€é•¿åº¦",
        "num_variants": "å˜ä½“æ•°é‡",
        "num_variants_info": "ç”Ÿæˆå¤šä¸ªæè¿°å˜ä½“",
        "custom_prompt_override": "è‡ªå®šä¹‰æç¤ºè¯ï¼ˆè¦†ç›–ç±»å‹é€‰æ‹©ï¼‰",
        "custom_prompt_placeholder": "åœ¨æ­¤è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰æç¤ºè¯...",
        "status": "çŠ¶æ€",
        "processing_time": "å¤„ç†æ—¶é—´",
        "generating": "â³ ç”Ÿæˆä¸­...",
        "stop_btn": "ğŸ›‘ åœæ­¢",
        "save_results": "ğŸ’¾ ä¿å­˜ç»“æœ",
        "output_folder": "è¾“å‡ºæ–‡ä»¶å¤¹åç§°",
        "output_folder_placeholder": "æˆ‘çš„æ•°æ®é›†",
        "export_format": "å¯¼å‡ºæ ¼å¼",
        "export_txt": "TXTï¼ˆæ¯å¼ å›¾ç‰‡ä¸€ä¸ªæ–‡ä»¶ï¼‰",
        "export_json": "JSONï¼ˆæ‰€æœ‰ç»“æœï¼‰",
        "export_csv": "CSVï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰",
        "variant": "å˜ä½“",
        "copy_btn": "ğŸ“‹ å¤åˆ¶",
        "download_btn": "â¬‡ï¸ ä¸‹è½½",
        "generation_complete": "âœ… ç”Ÿæˆå®Œæˆï¼",
        "seconds": "ç§’",
        "processing_image": "å¤„ç†å›¾åƒ",
        "of": "/",
        "extra_options": "é¢å¤–é€‰é¡¹",
        "extra_options_info": "æè¿°çš„é¢å¤–ä¿®é¥°ç¬¦",
        "character_name": "è§’è‰²/äººç‰©åç§°",
        "character_name_placeholder": "ä¾‹å¦‚ï¼šå°æ˜ã€å°çº¢ï¼Œæˆ–ç•™ç©º",
        "character_name_info": "å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨æ­¤åç§°ä»£æ›¿é€šç”¨æœ¯è¯­",
        "prompt_preset": "æç¤ºè¯é¢„è®¾",
        "prompt_preset_info": "ä»prompts/æ–‡ä»¶å¤¹åŠ è½½é¢„è®¾",
        "refresh_presets": "åˆ·æ–°",
        "memory_usage": "å†…å­˜ä½¿ç”¨",
        "download_result": "ä¸‹è½½ç»“æœ",
        "generation_stopped": "ç”¨æˆ·åœæ­¢äº†ç”Ÿæˆ",
        "stopping": "åœæ­¢ä¸­..."
    }
}

# Default language
current_language = "ru"

def get_text(key: str) -> str:
    """Get translated text for the current language"""
    return TRANSLATIONS[current_language].get(key, key)

def get_description_types() -> list:
    """Get description types for current language"""
    return list(DESCRIPTION_TYPES.get(current_language, DESCRIPTION_TYPES["en"]).keys())

def get_description_lengths() -> list:
    """Get description lengths for current language"""
    return list(DESCRIPTION_LENGTHS.get(current_language, DESCRIPTION_LENGTHS["en"]).keys())

def get_extra_options(is_video: bool = False) -> list:
    """Get extra options for current language and media type"""
    options_dict = EXTRA_OPTIONS_VIDEO if is_video else EXTRA_OPTIONS
    return list(options_dict.get(current_language, options_dict["en"]).keys())

def get_extra_option_prompt(option: str, is_video: bool = False) -> str:
    """Get the prompt text for an extra option"""
    options_dict = EXTRA_OPTIONS_VIDEO if is_video else EXTRA_OPTIONS
    lang_options = options_dict.get(current_language, options_dict["en"])
    return lang_options.get(option, "")

def build_prompt(
    description_type: str,
    description_length: str,
    custom_prompt: str,
    base_prompt: str = "",
    extra_options: list = None,
    character_name: str = "",
    is_video: bool = False
) -> str:
    """Build the final prompt based on type, length, custom input, extra options and character name.
    Automatically replaces 'image' with 'video' when is_video=True for context-aware prompts."""
    # If custom prompt is provided, use it (but still add character name if present)
    if custom_prompt and custom_prompt.strip():
        final_prompt = custom_prompt.strip()
    else:
        # Get type prompt
        types_dict = DESCRIPTION_TYPES.get(current_language, DESCRIPTION_TYPES["en"])
        type_prompt = types_dict.get(description_type, "")

        # If type is Custom, use base prompt
        if not type_prompt:
            if is_video:
                type_prompt = base_prompt if base_prompt else "Describe this video."
            else:
                type_prompt = base_prompt if base_prompt else "Describe this image."

        # Get length modifier
        lengths_dict = DESCRIPTION_LENGTHS.get(current_language, DESCRIPTION_LENGTHS["en"])
        # Fix if description_length is a list (shouldn't happen but Gradio bug)
        if isinstance(description_length, list):
            description_length = description_length[0] if description_length else ""
        length_modifier = lengths_dict.get(description_length, "")

        # Combine type and length
        if length_modifier:
            final_prompt = f"{type_prompt} {length_modifier}"
        else:
            final_prompt = type_prompt

    # Context-aware: Replace "image" with "video" when processing video
    if is_video:
        # English replacements
        final_prompt = final_prompt.replace(" image.", " video.")
        final_prompt = final_prompt.replace(" image ", " video ")
        final_prompt = final_prompt.replace("this image", "this video")
        final_prompt = final_prompt.replace("the image", "the video")
        final_prompt = final_prompt.replace("an image", "a video")
        # Russian replacements
        final_prompt = final_prompt.replace("Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ", "Ğ²Ğ¸Ğ´ĞµĞ¾")
        final_prompt = final_prompt.replace("Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸", "Ğ²Ğ¸Ğ´ĞµĞ¾")
        final_prompt = final_prompt.replace("Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ", "Ğ²Ğ¸Ğ´ĞµĞ¾")
        # Chinese replacements
        final_prompt = final_prompt.replace("å›¾ç‰‡", "è§†é¢‘")
        final_prompt = final_prompt.replace("å›¾åƒ", "è§†é¢‘")

    # Add character name instruction if provided
    if character_name and character_name.strip():
        name = character_name.strip()
        media_term_ru = "Ğ²Ğ¸Ğ´ĞµĞ¾" if is_video else "Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸"
        media_term_zh = "è§†é¢‘" if is_video else "å›¾ç‰‡"
        media_term_en = "video" if is_video else "image"

        if current_language == "ru":
            final_prompt += f" Ğ•ÑĞ»Ğ¸ Ğ½Ğ° {media_term_ru} ĞµÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ¼Ñ '{name}' Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²."
        elif current_language == "zh":
            final_prompt += f" å¦‚æœ{media_term_zh}ä¸­æœ‰äººæˆ–è§’è‰²ï¼Œè¯·ä½¿ç”¨åå­—'{name}'è€Œä¸æ˜¯é€šç”¨æœ¯è¯­ã€‚"
        else:
            final_prompt += f" If there is a person or character in the {media_term_en}, use the name '{name}' instead of generic terms."

    # Add extra options
    if extra_options:
        for option in extra_options:
            option_prompt = get_extra_option_prompt(option, is_video=is_video)
            if option_prompt:
                final_prompt += f" {option_prompt}"

    return final_prompt

def create_output_folder(folder_name: str = None) -> str:
    """Create and return path to output folder"""
    if folder_name and folder_name.strip():
        # Sanitize folder name
        safe_name = "".join(c for c in folder_name if c.isalnum() or c in "_ -").strip()
        if not safe_name:
            safe_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    else:
        safe_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    folder_path = os.path.join(DATASETS_DIR, safe_name)
    os.makedirs(folder_path, exist_ok=True)
    return folder_path

def save_results_txt(results: List[dict], output_folder: str) -> List[str]:
    """Save results as individual TXT files"""
    saved_files = []
    for result in results:
        filename = os.path.splitext(os.path.basename(result.get("image_path", "image")))[0]
        txt_path = os.path.join(output_folder, f"{filename}.txt")
        with open(txt_path, "w", encoding="utf-8") as f:
            if isinstance(result.get("descriptions"), list):
                for i, desc in enumerate(result["descriptions"], 1):
                    f.write(f"=== Variant {i} ===\n{desc}\n\n")
            else:
                f.write(result.get("description", ""))
        saved_files.append(txt_path)

        # Copy image if exists
        if result.get("image_path") and os.path.exists(result["image_path"]):
            img_dest = os.path.join(output_folder, os.path.basename(result["image_path"]))
            if not os.path.exists(img_dest):
                shutil.copy2(result["image_path"], img_dest)

    return saved_files

def save_results_json(results: List[dict], output_folder: str) -> str:
    """Save all results as a single JSON file"""
    json_path = os.path.join(output_folder, "results.json")
    export_data = []
    for result in results:
        export_data.append({
            "image": os.path.basename(result.get("image_path", "")),
            "prompt": result.get("prompt", ""),
            "descriptions": result.get("descriptions", [result.get("description", "")]),
            "timestamp": datetime.now().isoformat()
        })

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)

    return json_path

def save_results_csv(results: List[dict], output_folder: str) -> str:
    """Save all results as a CSV file"""
    csv_path = os.path.join(output_folder, "results.csv")

    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Image", "Prompt", "Description", "Variant"])

        for result in results:
            image_name = os.path.basename(result.get("image_path", ""))
            prompt = result.get("prompt", "")
            descriptions = result.get("descriptions", [result.get("description", "")])

            for i, desc in enumerate(descriptions, 1):
                writer.writerow([image_name, prompt, desc, i])

    return csv_path

class ImageDescriptionGenerator:
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_name = None
        self.current_quantization = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self, model_name: str, quantization: str = "4-bit"):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ BitsAndBytes"""
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
        if (self.current_model_name == model_name and
            self.current_quantization == quantization and
            self.model is not None):
            print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_name} ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
            return

        print(f"\n{'='*50}")
        print(f"ğŸ§  Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {model_name}")
        print(f"âš™ï¸ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: {quantization}")
        print(f"ğŸ’» Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {self.device}")
        print(f"{'='*50}")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ° Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        cached_size = get_model_cache_size(model_name)
        if cached_size:
            print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ² ĞºÑÑˆĞµ [{cached_size}]")
        else:
            print(f"â¬‡ï¸ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ² ĞºÑÑˆĞµ - Ğ±ÑƒĞ´ĞµÑ‚ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ° Ñ HuggingFace...")
            print(f"â³ Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ‚Ğ°Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        old_model = self.model
        old_processor = self.processor
        old_model_name = self.current_model_name
        old_quantization = self.current_quantization

        try:
            # ĞÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            if self.model is not None:
                print(f"ğŸ—‘ï¸ Ğ’Ñ‹Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {old_model_name}")
                self.model = None
                self.processor = None
                del old_model
                del old_processor
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                print(f"âœ… ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ°")

            # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ BitsAndBytes
            bnb_config = None
            dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

            if quantization == "4-bit" and torch.cuda.is_available():
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                print("âš¡ 4-bit ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (NF4) â€” ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ~75% VRAM")
            elif quantization == "8-bit" and torch.cuda.is_available():
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                )
                print("âš¡ 8-bit ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ â€” ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ~50% VRAM")
            else:
                print("ğŸ“Š ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (bfloat16/float32)")

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
            print(f"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
            load_start_time = time.time()

            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')
                load_kwargs = {
                    "device_map": "auto",
                    "trust_remote_code": True,
                }

                if bnb_config is not None:
                    load_kwargs["quantization_config"] = bnb_config
                else:
                    load_kwargs["torch_dtype"] = dtype

                # ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Flash Attention 2 Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸
                if torch.cuda.is_available():
                    try:
                        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Flash Attention 2
                        import flash_attn
                        load_kwargs["attn_implementation"] = "flash_attention_2"
                        print("ğŸš€ Flash Attention 2 (Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ)")
                    except ImportError:
                        # Fallback Ğ½Ğ° SDPA (Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ, Ğ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾)
                        load_kwargs["attn_implementation"] = "sdpa"
                        print("âš¡ SDPA attention (PyTorch native - Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ Flash Attention 2)")
                        print("ğŸ’¡ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ flash-attn Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ: pip install flash-attn --no-build-isolation")

                self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                    model_name,
                    **load_kwargs
                )

                # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ torch.compile (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ non-quantized Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
                # torch.compile Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ BitsAndBytes ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹
                if quantization == "ĞĞµÑ‚" and torch.cuda.is_available():
                    try:
                        print("ğŸ”¥ ĞšĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ torch.compile (ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ~20-40%)...")
                        # mode="reduce-overhead" Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ generative tasks
                        self.model.forward = torch.compile(
                            self.model.forward,
                            mode="reduce-overhead",
                            fullgraph=False
                        )
                        print("âœ… ĞšĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° (Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ)")
                    except Exception as e:
                        print(f"âš ï¸ torch.compile Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {e}")

                print(f"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°...")
                self.processor = AutoProcessor.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸
                    min_pixels=256*28*28,  # Min resolution
                    max_pixels=1280*28*28  # Max resolution (Ğ½Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ)
                )

            load_time = time.time() - load_start_time
            self.current_model_name = model_name
            self.current_quantization = quantization

            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
            memory_info = get_memory_info()
            print(f"\n{'='*50}")
            print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ·Ğ° {load_time:.1f} ÑĞµĞº")
            print(f"ğŸ“Š {memory_info}")
            print(f"{'='*50}\n")

        except Exception as e:
            # ĞŸÑ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ - Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {str(e)}")
            self.model = None
            self.processor = None
            self.current_model_name = None
            self.current_quantization = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise Exception(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {model_name}: {str(e)}")
    
    def _prepare_inputs(
        self,
        media_path: str,
        prompt: str,
        model_name: str,
        quantization: str,
        seed: int,
        is_video: bool = False,
        video_start_time = None,
        video_end_time = None
    ):
        """Prepare inputs for generation"""
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞµÑĞ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾
        self.load_model(model_name, quantization)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°
        if self.model is None or self.processor is None:
            raise Exception("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ.")

        # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ seed ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½
        if seed != -1:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        # Use "video" type for videos, "image" for images
        if is_video:
            content_item = {
                "type": "video",
                "video": media_path,
            }
            # Add video segment parameters if provided
            # Get video duration to validate end time
            video_duration = get_video_duration(media_path) if media_path else 0

            # video_start: only add if > 0
            if video_start_time is not None and video_start_time > 0:
                content_item["video_start"] = float(video_start_time)

            # video_end: only add if > 0 AND < video duration (otherwise means "to end")
            if video_end_time is not None and video_end_time > 0:
                # If video_end is close to duration (within 1 second), don't add (means full video)
                if video_duration > 0 and video_end_time < (video_duration - 1):
                    content_item["video_end"] = float(video_end_time)
        else:
            content_item = {
                "type": "image",
                "image": media_path,
            }

        messages = [
            {
                "role": "user",
                "content": [
                    content_item,
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ/Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)
        return inputs

    def _prepare_inputs_multi_image(
        self,
        image_paths: list,
        prompt: str,
        model_name: str,
        quantization: str,
        seed: int
    ):
        """Prepare inputs for multiple images generation"""
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞµÑĞ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾
        self.load_model(model_name, quantization)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°
        if self.model is None or self.processor is None:
            raise Exception("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ.")

        # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ seed ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½
        if seed != -1:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸
        content = []
        for img_path in image_paths:
            content.append({
                "type": "image",
                "image": img_path,
            })

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ² ĞºĞ¾Ğ½ĞµÑ†
        content.append({"type": "text", "text": prompt})

        messages = [
            {
                "role": "user",
                "content": content,
            }
        ]

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)
        return inputs

    def generate_description(
        self,
        image_path: str,
        prompt: str,
        model_name: str,
        quantization: str = "4-bit",
        max_new_tokens: int = 1024,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 50,
        seed: int = -1,
        is_video: bool = False,
        video_start_time = None,
        video_end_time = None
    ) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ/Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ±ĞµĞ· streaming)"""
        try:
            inputs = self._prepare_inputs(
                image_path, prompt, model_name, quantization, seed, is_video,
                video_start_time, video_end_time
            )

            # Non-streaming generation (inference_mode Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ‡ĞµĞ¼ no_grad)
            with torch.inference_mode():
                # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
                gen_kwargs = {
                    **inputs,
                    "max_new_tokens": max_new_tokens,
                    "use_cache": True,  # KV ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ
                    "pad_token_id": self.processor.tokenizer.pad_token_id,
                }

                # Greedy decoding Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ sampling
                if temperature > 0:
                    gen_kwargs.update({
                        "do_sample": True,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k,
                    })
                else:
                    gen_kwargs["do_sample"] = False

                generated_ids = self.model.generate(**gen_kwargs)

            # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )

            return output_text[0]

        except Exception as e:
            error_msg = get_text("error_generation").format(str(e))
            return error_msg

    def generate_description_stream(
        self,
        image_path: str,
        prompt: str,
        model_name: str,
        quantization: str = "4-bit",
        max_new_tokens: int = 1024,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 50,
        seed: int = -1,
        is_video: bool = False,
        video_start_time = None,
        video_end_time = None
    ) -> Generator[str, None, None]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ/Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ streaming"""
        try:
            inputs = self._prepare_inputs(
                image_path, prompt, model_name, quantization, seed, is_video,
                video_start_time, video_end_time
            )

            streamer = TextIteratorStreamer(
                self.processor.tokenizer,
                skip_special_tokens=True,
                skip_prompt=True
            )

            generation_kwargs = {
                **inputs,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "do_sample": True if temperature > 0 else False,
                "use_cache": True,
                "streamer": streamer,
            }

            # Run generation in a separate thread
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()

            # Yield tokens as they come
            generated_text = ""
            for new_text in streamer:
                generated_text += new_text
                yield generated_text

            thread.join()

        except Exception as e:
            error_msg = get_text("error_generation").format(str(e))
            yield error_msg

    def generate_description_multi_image_stream(
        self,
        image_paths: list,
        prompt: str,
        model_name: str,
        quantization: str = "4-bit",
        max_new_tokens: int = 2048,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 50,
        seed: int = -1
    ) -> Generator[str, None, None]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ streaming"""
        try:
            inputs = self._prepare_inputs_multi_image(
                image_paths, prompt, model_name, quantization, seed
            )

            streamer = TextIteratorStreamer(
                self.processor.tokenizer,
                skip_special_tokens=True,
                skip_prompt=True
            )

            generation_kwargs = {
                **inputs,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "do_sample": True if temperature > 0 else False,
                "use_cache": True,
                "streamer": streamer,
            }

            # Run generation in a separate thread
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()

            # Yield tokens as they come
            generated_text = ""
            for new_text in streamer:
                generated_text += new_text
                yield generated_text

            thread.join()

        except Exception as e:
            error_msg = get_text("error_generation").format(str(e))
            yield error_msg

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
generator = ImageDescriptionGenerator()

def process_single_image(
    image,
    video,
    video_start_time,
    video_end_time,
    description_type: str,
    description_length: str,
    custom_prompt: str,
    extra_options: list,
    character_name: str,
    num_variants: int,
    model_name: str,
    quantization: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    seed: int,
    use_streaming: bool = True,
    progress=gr.Progress(track_tqdm=True)
) -> Generator:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ/Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¸ streaming output"""
    global stop_generation_flag
    reset_stop_flag()
    start_time = time.time()

    # Start capturing console output
    log_capture.clear_logs()
    log_capture.start_capture()
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸...")

    # Check if we have either uploaded image or video
    if image is None and video is None:
        yield get_text("error_no_image"), "", [], None, log_capture.get_logs(), "", None
        return

    # Determine if processing video
    is_video = video is not None

    # Build prompt from type, length, custom, extra options and character name
    final_prompt = build_prompt(
        description_type, description_length, custom_prompt,
        extra_options=extra_options or [],
        character_name=character_name or "",
        is_video=is_video
    )
    if not final_prompt.strip():
        yield get_text("error_no_prompt"), "", [], None, log_capture.get_logs(), "", None
        return

    temp_path = None
    num_variants = int(num_variants) if num_variants and str(num_variants).strip() else 1

    try:
        # Priority: video > image
        if video is not None:
            # Video uploaded - use it directly
            media_path = video
            is_video = True
        elif hasattr(image, 'shape'):
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ numpy array
            temp_path = os.path.join(TEMP_DIR, "temp_image.jpg")
            Image.fromarray(image).save(temp_path)
            media_path = temp_path
            is_video = False
        else:
            media_path = image
            is_video = False

        results = []
        full_results = []  # Store full results before parsing
        reasoning_list = []  # Store reasoning from thinking models
        variant_times = []  # Track time for each variant
        image_preview_path = None  # Path to image with bboxes drawn

        for i in range(num_variants):
            # Check stop flag
            if stop_generation_flag:
                elapsed_time = time.time() - start_time
                print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼")
                yield f"ğŸ›‘ {get_text('generation_stopped')} ({get_text('processing_time')}: {elapsed_time:.1f} {get_text('seconds')})", final_prompt, results, None, log_capture.get_logs(), "", None
                return

            variant_start = time.time()
            variant_seed = seed if seed == -1 else seed + i
            memory_info = get_memory_info()
            status_msg = f"{get_text('generating')} ({get_text('variant')} {i+1}/{num_variants}) | {memory_info}"
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° {i+1}/{num_variants}, seed={variant_seed}")

            if use_streaming:
                # Stream the generation for real-time output
                current_result = ""
                for partial_result in generator.generate_description_stream(
                    image_path=media_path,
                    prompt=final_prompt,
                    model_name=model_name,
                    quantization=quantization,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    seed=variant_seed,
                    is_video=is_video,
                    video_start_time=video_start_time,
                    video_end_time=video_end_time
                ):
                    if stop_generation_flag:
                        break
                    current_result = partial_result
                    # Update results with streaming text
                    temp_results = results + [current_result]
                    yield status_msg, final_prompt, temp_results, None, log_capture.get_logs(), "", None

                result = current_result
            else:
                # Non-streaming generation
                yield status_msg, final_prompt, results, None, log_capture.get_logs(), "", None
                result = generator.generate_description(
                    image_path=media_path,
                    prompt=final_prompt,
                    model_name=model_name,
                    quantization=quantization,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    seed=variant_seed,
                    is_video=is_video,
                    video_start_time=video_start_time,
                    video_end_time=video_end_time
                )

            variant_time = time.time() - variant_start
            variant_times.append(variant_time)

            # Store full result before parsing
            full_results.append(result)

            # Parse thinking output if present
            reasoning, final_answer = parse_thinking_output(result)
            reasoning_list.append(reasoning)  # Store reasoning (empty string if no thinking)
            results.append(final_answer)  # Store only final answer in results

            print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ {i+1} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ Ğ·Ğ° {variant_time:.1f}s")

        # Calculate processing time
        elapsed_time = time.time() - start_time
        memory_info = get_memory_info()

        # Check for thinking output and bounding boxes in results
        total_bboxes = 0
        has_thinking = any(r for r in reasoning_list if r)  # Check if any reasoning exists
        first_bbox_result = None

        for i, full_result in enumerate(full_results):
            # Log thinking if present
            if reasoning_list[i]:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’­ Variant {i+1}: Thinking model detected")
                print(f"  Reasoning length: {len(reasoning_list[i])} chars")
                print(f"  Answer length: {len(results[i])} chars")

            # Check for bounding boxes in full result
            bboxes = parse_bboxes_from_text(full_result)
            if bboxes:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Variant {i+1}: Found {len(bboxes)} bounding boxes")
                for j, bbox in enumerate(bboxes):
                    label = bbox.get('label', 'unknown')
                    coords = bbox.get('bbox_2d', [])
                    print(f"  - Object {j+1}: {label} at {coords}")
                total_bboxes += len(bboxes)

                # Draw bboxes on image for first variant with bboxes
                if first_bbox_result is None and not is_video and media_path:
                    try:
                        image_preview_path = draw_bboxes_on_image(media_path, bboxes, normalized=True)
                        first_bbox_result = i
                        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Drew {len(bboxes)} bboxes on image: {image_preview_path}")
                    except Exception as e:
                        print(f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ Failed to draw bboxes: {e}")

        # Build detailed status with per-variant timing
        timing_details = " | ".join([f"V{i+1}: {t:.1f}s" for i, t in enumerate(variant_times)])
        thinking_info = " | ğŸ’­ Thinking" if has_thinking else ""
        bbox_info = f" | ğŸ“ {total_bboxes} bbox" if total_bboxes > 0 else ""
        final_status = f"{get_text('generation_complete')} | Total: {elapsed_time:.1f}s ({timing_details}){thinking_info}{bbox_info} | {memory_info}"

        # Prepare download file
        download_path = None
        if results:
            all_text = "\n\n".join([f"=== Variant {i+1} (Time: {variant_times[i]:.1f}s) ===\n{r}" for i, r in enumerate(results)])
            download_path = save_text_to_file(all_text, f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

        # Get first reasoning (or empty string)
        first_reasoning = reasoning_list[0] if reasoning_list and reasoning_list[0] else ""

        print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ° {elapsed_time:.1f}s")
        yield final_status, final_prompt, results, download_path, log_capture.get_logs(), first_reasoning, image_preview_path

    except Exception as e:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")
        yield f"âŒ Error: {str(e)}", final_prompt, [], None, log_capture.get_logs(), "", None
    finally:
        # Stop capturing console output
        log_capture.stop_capture()

        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass

        # Clean up GPU memory
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def process_multi_image(
    multi_images_list,
    description_type: str,
    description_length: str,
    custom_prompt: str,
    extra_options: list,
    character_name: str,
    num_variants: int,
    model_name: str,
    quantization: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    seed: int,
    use_streaming: bool = True,
    progress=gr.Progress(track_tqdm=True)
) -> Generator:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¸ streaming output"""
    global stop_generation_flag
    reset_stop_flag()
    start_time = time.time()

    # Start capturing console output
    log_capture.clear_logs()
    log_capture.start_capture()
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹...")

    # Check if we have images
    if not multi_images_list or len(multi_images_list) == 0:
        yield "âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ", "", [], None, log_capture.get_logs()
        return

    # Extract file paths from Gallery format
    if isinstance(multi_images_list, list):
        # Gallery returns list of dicts or tuples or strings
        image_paths = []
        for item in multi_images_list:
            if isinstance(item, dict) and 'name' in item:
                image_paths.append(item['name'])
            elif isinstance(item, tuple):
                image_paths.append(item[0])
            elif isinstance(item, str):
                image_paths.append(item)
            else:
                image_paths.append(str(item))
    else:
        yield "âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "", [], None, log_capture.get_logs()
        return

    num_images = len(image_paths)
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {num_images} Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")

    # Limit to 20 images max
    if num_images > 20:
        yield f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 20 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {num_images}", "", [], None, log_capture.get_logs()
        return

    # Build prompt from type, length, custom, extra options and character name
    final_prompt = build_prompt(
        description_type, description_length, custom_prompt,
        extra_options=extra_options or [],
        character_name=character_name or "",
        is_video=False  # Always false for multi-image
    )
    if not final_prompt.strip():
        yield "âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹", "", [], None, log_capture.get_logs()
        return

    num_variants = int(num_variants) if num_variants and str(num_variants).strip() else 1

    try:
        results = []
        variant_times = []

        for i in range(num_variants):
            # Check stop flag
            if stop_generation_flag:
                elapsed_time = time.time() - start_time
                print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼")
                yield f"ğŸ›‘ {get_text('generation_stopped')} ({get_text('processing_time')}: {elapsed_time:.1f} {get_text('seconds')})", final_prompt, results, None, log_capture.get_logs(), "", None
                return

            variant_start = time.time()
            variant_seed = seed if seed == -1 else seed + i
            memory_info = get_memory_info()
            status_msg = f"{get_text('generating')} ({get_text('variant')} {i+1}/{num_variants}) | {num_images} Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ | {memory_info}"
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° {i+1}/{num_variants}, seed={variant_seed}")

            if use_streaming:
                # Stream the generation for real-time output
                current_result = ""
                for partial_result in generator.generate_description_multi_image_stream(
                    image_paths=image_paths,
                    prompt=final_prompt,
                    model_name=model_name,
                    quantization=quantization,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    seed=variant_seed
                ):
                    if stop_generation_flag:
                        break
                    current_result = partial_result
                    # Update results with streaming text
                    temp_results = results + [current_result]
                    yield status_msg, final_prompt, temp_results, None, log_capture.get_logs()

                result = current_result
            else:
                # Non-streaming generation (not implemented yet, use streaming)
                yield status_msg, final_prompt, results, None, log_capture.get_logs()
                # Fallback to streaming
                for partial_result in generator.generate_description_multi_image_stream(
                    image_paths=image_paths,
                    prompt=final_prompt,
                    model_name=model_name,
                    quantization=quantization,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    seed=variant_seed
                ):
                    result = partial_result

            variant_time = time.time() - variant_start
            variant_times.append(variant_time)
            results.append(result)
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ {i+1} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ Ğ·Ğ° {variant_time:.1f}s")

        # Calculate processing time
        elapsed_time = time.time() - start_time
        memory_info = get_memory_info()

        # Check for thinking output and bounding boxes in results
        total_bboxes = 0
        has_thinking = False
        for i, result in enumerate(results):
            # Check for thinking output
            reasoning, final_answer = parse_thinking_output(result)
            if reasoning:
                has_thinking = True
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ’­ Variant {i+1}: Thinking model detected")
                print(f"  Reasoning length: {len(reasoning)} chars")
                print(f"  Answer length: {len(final_answer)} chars")

            # Check for bounding boxes
            bboxes = parse_bboxes_from_text(result)
            if bboxes:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“ Variant {i+1}: Found {len(bboxes)} bounding boxes")
                for j, bbox in enumerate(bboxes):
                    label = bbox.get('label', 'unknown')
                    coords = bbox.get('bbox_2d', [])
                    print(f"  - Object {j+1}: {label} at {coords}")
                total_bboxes += len(bboxes)

        # Build detailed status with per-variant timing
        timing_details = " | ".join([f"V{i+1}: {t:.1f}s" for i, t in enumerate(variant_times)])
        thinking_info = " | ğŸ’­ Thinking" if has_thinking else ""
        bbox_info = f" | ğŸ“ {total_bboxes} bbox" if total_bboxes > 0 else ""
        final_status = f"{get_text('generation_complete')} | Total: {elapsed_time:.1f}s ({timing_details}){thinking_info}{bbox_info} | {memory_info}"

        # Prepare download file
        download_path = None
        if results:
            all_text = "\n\n".join([f"=== Variant {i+1} (Time: {variant_times[i]:.1f}s) ===\n{r}" for i, r in enumerate(results)])
            download_path = save_text_to_file(all_text, f"multi_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

        print(f"[{datetime.now().strftime('%H:%M:%S')}] Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ° {elapsed_time:.1f}s")
        yield final_status, final_prompt, results, download_path, log_capture.get_logs()

    except Exception as e:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ĞÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")
        yield f"âŒ Error: {str(e)}", final_prompt, [], None, log_capture.get_logs()
    finally:
        # Stop capturing console output
        log_capture.stop_capture()

        # Clean up GPU memory
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def process_batch_images(
    files: List,
    description_type: str,
    description_length: str,
    custom_prompt: str,
    extra_options: list,
    character_name: str,
    num_variants: int,
    output_folder_name: str,
    export_formats: List[str],
    model_name: str,
    quantization: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    seed: int,
    is_video: bool = False,
    progress=gr.Progress(track_tqdm=True)
) -> Generator:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ğ°ĞºĞµÑ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹/Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ¾Ğ¼ Ğ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ¼"""
    global stop_generation_flag
    reset_stop_flag()
    start_time = time.time()

    if not files:
        yield get_text("error_no_images"), "", None
        return

    # Build prompt from type, length, custom, extra options and character name
    final_prompt = build_prompt(
        description_type, description_length, custom_prompt,
        extra_options=extra_options or [],
        character_name=character_name or "",
        is_video=is_video
    )
    if not final_prompt.strip():
        yield get_text("error_no_prompts"), "", None
        return

    num_variants = int(num_variants)
    total_files = len(files)
    all_results = []
    output_lines = []

    # Create output folder
    output_folder = create_output_folder(output_folder_name)

    media_type = "video" if is_video else get_text('processing_image')

    for idx, file in enumerate(progress.tqdm(files, desc=f"Processing {media_type}s")):
        # Check stop flag
        if stop_generation_flag:
            elapsed_time = time.time() - start_time
            final_status = f"ğŸ›‘ {get_text('generation_stopped')}\n"
            final_status += f"ğŸ“Š {idx} {get_text('of')} {total_files} files processed in {elapsed_time:.1f} {get_text('seconds')}"
            yield final_status, "\n".join(output_lines), None
            return

        image_path = file.name if hasattr(file, 'name') else file
        filename = os.path.basename(image_path)

        # Status update with memory info
        memory_info = get_memory_info()
        status_msg = f"â³ Processing {idx + 1} {get_text('of')} {total_files}: {filename} | {memory_info}"
        yield status_msg, "\n".join(output_lines), None

        descriptions = []
        variant_times = []

        for v in range(num_variants):
            # Check stop flag between variants
            if stop_generation_flag:
                break

            variant_start = time.time()
            variant_seed = seed if seed == -1 else seed + idx * num_variants + v

            result = generator.generate_description(
                image_path=image_path,
                prompt=final_prompt,
                model_name=model_name,
                quantization=quantization,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                seed=variant_seed,
                is_video=is_video
            )
            variant_time = time.time() - variant_start
            variant_times.append(variant_time)
            descriptions.append(result)

        if not descriptions:
            continue

        # Store result
        all_results.append({
            "image_path": image_path,
            "prompt": final_prompt,
            "descriptions": descriptions,
            "description": descriptions[0] if descriptions else ""
        })

        # Add to output with timing info
        output_lines.append(f"{'='*50}")
        output_lines.append(get_text("image_label").format(idx + 1, filename))
        output_lines.append(get_text("prompt_label").format(final_prompt))

        for v_idx, desc in enumerate(descriptions, 1):
            if num_variants > 1:
                timing_info = f" ({variant_times[v_idx-1]:.1f}s)" if v_idx-1 < len(variant_times) else ""
                output_lines.append(f"\n--- {get_text('variant')} {v_idx}{timing_info} ---")
            output_lines.append(f"{desc}\n")

        yield status_msg, "\n".join(output_lines), None

    # Save results in selected formats
    saved_paths = []
    if export_formats and all_results:
        if "TXT" in export_formats:
            txt_files = save_results_txt(all_results, output_folder)
            saved_paths.extend(txt_files)

        if "JSON" in export_formats:
            json_path = save_results_json(all_results, output_folder)
            saved_paths.append(json_path)

        if "CSV" in export_formats:
            csv_path = save_results_csv(all_results, output_folder)
            saved_paths.append(csv_path)

    # Calculate total time
    elapsed_time = time.time() - start_time
    memory_info = get_memory_info()

    # Final status
    final_status = f"{get_text('generation_complete')}\n"
    final_status += f"ğŸ“Š {total_files} images processed in {elapsed_time:.1f} {get_text('seconds')} | {memory_info}\n"
    if saved_paths:
        final_status += f"ğŸ’¾ Results saved to: {output_folder}"

    # Prepare download file for batch results
    download_path = None
    if all_results:
        download_path = save_text_to_file("\n".join(output_lines), f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

    yield final_status, "\n".join(output_lines), download_path

    # Clean up GPU memory
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def random_seed() -> int:
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ seed"""
    return random.randint(0, 2**32 - 1)

def update_examples():
    return [
        [get_text("example_1")],
        [get_text("example_2")],
        [get_text("example_3")],
        [get_text("example_4")]
    ]

def update_video_sliders(video_path):
    """Update video time sliders based on actual video duration"""
    if not video_path:
        return gr.update(maximum=7200), gr.update(maximum=7200)

    duration = get_video_duration(video_path)
    # Round up to nearest 10 seconds for cleaner UI
    max_duration = ((int(duration) + 9) // 10) * 10

    return (
        gr.update(maximum=max_duration, value=0),
        gr.update(maximum=max_duration, value=max_duration)
    )

def create_interface():
    """Create Gradio interface with current language and beautiful styling"""
    with gr.Blocks(
        title="SuperCaption Qwen3-VL NSFW",
        theme=gr.themes.Soft(primary_hue="indigo", secondary_hue="purple", neutral_hue="slate"),
        css=CUSTOM_CSS
    ) as demo:
        # Beautiful gradient header with credits
        gr.HTML("""
        <div class="main-header">
            <h1>ğŸ¬ SuperCaption Qwen3-VL NSFW</h1>
            <p>Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞ³Ğ¾Ğ² Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾</p>
            <p style="font-size: 0.85rem; margin-top: 0.5rem; opacity: 0.9;">
                ğŸ”“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ <b>Abliterated</b> Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ±ĞµĞ· Ñ†ĞµĞ½Ğ·ÑƒÑ€Ñ‹
            </p>
            <p style="font-size: 0.9rem; margin-top: 0.75rem; opacity: 0.85;">
                ĞŸĞ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¾Ñ‚ <a href="https://t.me/nerual_dreming" target="_blank">Nerual Dreming</a> Ğ¸
                <a href="https://t.me/ruweb24" target="_blank">Slait</a> |
                Ğ¢ĞµĞ»ĞµĞ³Ñ€Ğ°Ğ¼ ĞºĞ°Ğ½Ğ°Ğ»: <a href="https://t.me/neuroport" target="_blank">ğŸ‘¾ ĞĞ•Ğ™Ğ Ğ-Ğ¡ĞĞ¤Ğ¢</a>
            </p>
        </div>
        """)

        # Header that will be updated for language
        header_md = gr.Markdown(f"""
        {get_text("subtitle")}
        """)
        
        # ĞĞ±Ñ‰Ğ¸Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹Ğº
        with gr.Row():
            model_dropdown = gr.Dropdown(
                choices=get_model_choices(),
                value="huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated",
                label=get_text("model_selection"),
                info=get_text("model_info"),
                scale=3
            )
            refresh_models_btn = gr.Button(
                "ğŸ”„",
                size="sm",
                scale=0,
                min_width=40
            )
            quantization_dropdown = gr.Dropdown(
                choices=[
                    ("4-bit (Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)", "4-bit"),
                    ("8-bit (Ğ›ÑƒÑ‡ÑˆĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾)", "8-bit"),
                    ("Ğ‘ĞµĞ· (ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ)", "none"),
                ],
                value="4-bit",
                label=get_text("quantization"),
                info=get_text("quantization_info"),
                scale=1
            )
            language_dropdown = gr.Dropdown(
                choices=[("English", "en"), ("Ğ ÑƒÑÑĞºĞ¸Ğ¹", "ru"), ("ä¸­æ–‡", "zh")],
                value=current_language,
                label=get_text("language"),
                info=get_text("language_info"),
                scale=1
            )

        # Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
        advanced_accordion = gr.Accordion(get_text("advanced_params"), open=False)
        with advanced_accordion:
            with gr.Row():
                max_tokens_slider = gr.Slider(
                    minimum=1,
                    maximum=4096,
                    value=1024,
                    step=1,
                    label=get_text("max_tokens"),
                    info=get_text("max_tokens_info")
                )
                temperature_slider = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.6,
                    step=0.1,
                    label=get_text("temperature"),
                    info=get_text("temperature_info")
                )
            
            with gr.Row():
                top_p_slider = gr.Slider(
                    minimum=0.05,
                    maximum=1.0,
                    value=0.9,
                    step=0.05,
                    label=get_text("top_p"),
                    info=get_text("top_p_info")
                )
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=1000,
                    value=50,
                    step=1,
                    label=get_text("top_k"),
                    info=get_text("top_k_info")
                )
            
            with gr.Row():
                seed_number = gr.Number(
                    value=-1,
                    label=get_text("seed"),
                    info=get_text("seed_info"),
                    precision=0
                )
                random_seed_btn = gr.Button(get_text("random_seed_btn"), size="sm")
        
        # Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        tabs = gr.Tabs()
        with tabs:
            # Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
            single_tab = gr.TabItem(get_text("single_processing"))
            with single_tab:
                with gr.Row():
                    with gr.Column(scale=1, elem_classes="card-style"):
                        gr.Markdown("### ğŸ“· Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")

                        # Track which media tab is active
                        active_media_type = gr.State("image")  # Default to image tab

                        # Tabs for Image and Video
                        media_tabs = gr.Tabs()
                        with media_tabs:
                            image_tab = gr.TabItem("ğŸ–¼ï¸ Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ")
                            with image_tab:
                                single_image = gr.Image(
                                    type="numpy",
                                    label=get_text("upload_image"),
                                    height=300
                                )
                                # Duplicate Generate/Stop buttons for quick access
                                with gr.Row():
                                    single_generate_btn_image = gr.Button(
                                        get_text("generate_btn"),
                                        variant="primary",
                                        elem_classes="generate-btn",
                                        scale=3
                                    )
                                    single_stop_btn_image = gr.Button(
                                        get_text("stop_btn"),
                                        variant="stop",
                                        scale=1,
                                        interactive=False
                                    )

                            video_tab = gr.TabItem("ğŸ¥ Ğ’Ğ¸Ğ´ĞµĞ¾")
                            with video_tab:
                                single_video = gr.Video(
                                    label="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                                    height=300
                                )
                                # Video segment controls
                                with gr.Row():
                                    video_start_time = gr.Slider(
                                        label="â±ï¸ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ (ÑĞµĞºÑƒĞ½Ğ´Ñ‹)",
                                        value=0,
                                        minimum=0,
                                        maximum=7200,
                                        step=0.1,
                                        info="ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ (0 = Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°)"
                                    )
                                    video_end_time = gr.Slider(
                                        label="â±ï¸ ĞšĞ¾Ğ½ĞµÑ† (ÑĞµĞºÑƒĞ½Ğ´Ñ‹)",
                                        value=7200,
                                        minimum=0,
                                        maximum=7200,
                                        step=0.1,
                                        info="ĞšĞ¾Ğ½ĞµÑ† ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ = Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ°)"
                                    )
                                # Duplicate Generate/Stop buttons for quick access
                                with gr.Row():
                                    single_generate_btn_video = gr.Button(
                                        get_text("generate_btn"),
                                        variant="primary",
                                        elem_classes="generate-btn",
                                        scale=3
                                    )
                                    single_stop_btn_video = gr.Button(
                                        get_text("stop_btn"),
                                        variant="stop",
                                        scale=1,
                                        interactive=False
                                    )

                            multi_image_tab = gr.TabItem("ğŸ–¼ï¸âœ• ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")
                            with multi_image_tab:
                                multi_images = gr.Gallery(
                                    label="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ´Ğ¾ 20)",
                                    columns=4,
                                    rows=3,
                                    height=400,
                                    allow_preview=True,
                                    show_label=True,
                                    type="filepath",
                                    object_fit="contain",
                                    show_download_button=False
                                )
                                gr.Markdown("ğŸ’¡ **ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°:** Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ 2-20 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°")
                                # Duplicate Generate/Stop buttons for quick access
                                with gr.Row():
                                    single_generate_btn_multi = gr.Button(
                                        get_text("generate_btn"),
                                        variant="primary",
                                        elem_classes="generate-btn",
                                        scale=3
                                    )
                                    single_stop_btn_multi = gr.Button(
                                        get_text("stop_btn"),
                                        variant="stop",
                                        scale=1,
                                        interactive=False
                                    )

                        gr.Markdown("### ğŸ“ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ")

                        single_desc_type = gr.Dropdown(
                            choices=get_description_types(),
                            value=get_description_types()[0],
                            label=get_text("description_type"),
                            info=get_text("description_type_info")
                        )
                        single_desc_length = gr.Dropdown(
                            choices=get_description_lengths(),
                            value=get_description_lengths()[0],
                            label=get_text("description_length"),
                            info=get_text("description_length_info")
                        )
                        single_num_variants = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=1,
                            step=1,
                            label=get_text("num_variants"),
                            info=get_text("num_variants_info")
                        )

                        # Custom prompt - visible by default
                        single_custom_prompt = gr.Textbox(
                            label=get_text("custom_prompt_override"),
                            placeholder=get_text("custom_prompt_placeholder"),
                            lines=3
                        )

                        # Extra options (will update based on media type)
                        with gr.Accordion(get_text("extra_options"), open=False):
                            single_extra_options = gr.CheckboxGroup(
                                choices=get_extra_options(is_video=False),  # Default to image
                                value=[],
                                label="",
                                info=get_text("extra_options_info"),
                                elem_id="single_extra_options"
                            )
                            # Hidden state to track if video is selected
                            single_is_video = gr.State(value=False)

                            # Character name field - hidden in accordion
                            single_character_name = gr.Textbox(
                                label=get_text("character_name"),
                                placeholder=get_text("character_name_placeholder"),
                                info=get_text("character_name_info"),
                                lines=1
                            )

                        # Prompt presets section - collapsed accordion at the bottom
                        with gr.Accordion("ğŸ“‹ ĞŸÑ€ĞµÑĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²", open=False):
                            with gr.Row():
                                single_preset = gr.Dropdown(
                                    choices=list(load_prompt_presets().keys()),
                                    value="None",
                                    label=get_text("prompt_preset"),
                                    info=get_text("prompt_preset_info"),
                                    scale=3
                                )
                                single_refresh_presets = gr.Button(
                                    "ğŸ”„",
                                    size="sm",
                                    scale=0,
                                    min_width=40
                                )
                            gr.Markdown("---")
                            gr.Markdown("**Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ ĞºĞ°Ğº Ğ¿Ñ€ĞµÑĞµÑ‚:**")
                            with gr.Row():
                                single_save_preset_name = gr.Textbox(
                                    label="Ğ˜Ğ¼Ñ Ğ¿Ñ€ĞµÑĞµÑ‚Ğ°",
                                    placeholder="Ğ¼Ğ¾Ğ¹_Ğ¿Ñ€ĞµÑĞµÑ‚",
                                    scale=2
                                )
                                single_save_preset_btn = gr.Button(
                                    "ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ",
                                    size="sm",
                                    scale=1
                                )
                            single_save_preset_status = gr.Markdown("")

                        with gr.Row():
                            single_submit_btn = gr.Button(
                                get_text("generate_btn"),
                                variant="primary",
                                elem_classes="generate-btn",
                                scale=3
                            )
                            single_stop_btn = gr.Button(
                                get_text("stop_btn"),
                                variant="stop",
                                scale=1,
                                interactive=False
                            )

                    with gr.Column(scale=1, elem_classes="card-style"):
                        gr.Markdown("### ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
                        single_status = gr.Textbox(
                            label=get_text("status"),
                            interactive=False,
                            elem_classes="status-box"
                        )

                        single_prompt_used = gr.Textbox(
                            label="Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚",
                            interactive=True,
                            lines=4,
                            info="ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ½Ğ¾Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
                            show_copy_button=True
                        )

                        # Button to copy prompt back to custom prompt
                        use_prompt_btn = gr.Button(
                            "â¬†ï¸ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚",
                            size="sm",
                            variant="secondary"
                        )

                        # Thinking Process section (collapsible)
                        with gr.Accordion("ğŸ’­ ĞœÑ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ", open=True, visible=True) as thinking_accordion:
                            thinking_output = gr.Textbox(
                                label="",
                                lines=6,
                                interactive=False,
                                show_copy_button=True,
                                placeholder="Ğ—Ğ´ĞµÑÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ´Ğ»Ñ Thinking Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)..."
                            )

                        # Image preview with bboxes
                        with gr.Accordion("ğŸ–¼ï¸ ĞŸÑ€ĞµĞ²ÑŒÑ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ", open=True, visible=True) as image_preview_accordion:
                            image_preview = gr.Image(
                                label="",
                                type="filepath",
                                interactive=False,
                                show_label=False
                            )

                        # Multiple variant outputs
                        single_outputs = []
                        for i in range(5):
                            with gr.Group(visible=(i == 0)) as variant_group:
                                variant_output = gr.Textbox(
                                    label=f"{get_text('variant')} {i+1}" if i > 0 else get_text("result"),
                                    lines=8,
                                    show_copy_button=True
                                )
                                single_outputs.append((variant_group, variant_output))

                        # Download result file
                        single_download = gr.File(
                            label=get_text("download_result"),
                            visible=True
                        )

                        # Console output accordion
                        with gr.Accordion("ğŸ“Ÿ ĞšĞ¾Ğ½ÑĞ¾Ğ»ÑŒ", open=False):
                            single_console_output = gr.Textbox(
                                label="",
                                lines=10,
                                max_lines=20,
                                interactive=False,
                                show_copy_button=True,
                                placeholder="Ğ—Ğ´ĞµÑÑŒ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸..."
                            )

                # ĞšĞ»Ğ¸ĞºĞ°Ğ±ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ğ¾Ğ²
                examples_title = gr.Markdown(f"### {get_text('examples_title')}")

            # Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ - Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾
            batch_tab = gr.TabItem(get_text("batch_processing"))
            with batch_tab:
                # Sub-tabs for images and videos
                batch_media_tabs = gr.Tabs()
                with batch_media_tabs:
                    # BATCH IMAGES TAB
                    batch_images_tab = gr.TabItem("ğŸ“š ĞŸĞ°ĞºĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")
                    with batch_images_tab:
                        with gr.Row():
                            with gr.Column(scale=1, elem_classes="card-style"):
                                gr.Markdown("### ğŸ“ Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹")
                                batch_images = gr.File(
                                    file_count="multiple",
                                    label=get_text("upload_images"),
                                    file_types=["image"]
                                )

                                gr.Markdown("### ğŸ“ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ")

                                # Prompt preset dropdown
                                with gr.Row():
                                    batch_preset = gr.Dropdown(
                                        choices=list(load_prompt_presets().keys()),
                                        value="None",
                                        label=get_text("prompt_preset"),
                                        info=get_text("prompt_preset_info"),
                                        scale=4
                                    )
                                    batch_refresh_presets = gr.Button(
                                        "ğŸ”„",
                                        size="sm",
                                        scale=0,
                                        min_width=40
                                    )

                                batch_desc_type = gr.Dropdown(
                                    choices=get_description_types(),
                                    value=get_description_types()[0],
                                    label=get_text("description_type"),
                                    info=get_text("description_type_info")
                                )
                                batch_desc_length = gr.Dropdown(
                                    choices=get_description_lengths(),
                                    value=get_description_lengths()[0],
                                    label=get_text("description_length"),
                                    info=get_text("description_length_info")
                                )
                                batch_num_variants = gr.Slider(
                                    minimum=1,
                                    maximum=3,
                                    value=1,
                                    step=1,
                                    label=get_text("num_variants"),
                                    info=get_text("num_variants_info")
                                )

                                # Character name field
                                batch_character_name = gr.Textbox(
                                    label=get_text("character_name"),
                                    placeholder=get_text("character_name_placeholder"),
                                    info=get_text("character_name_info"),
                                    lines=1
                                )

                                # Extra options for images
                                with gr.Accordion(get_text("extra_options"), open=False):
                                    batch_extra_options = gr.CheckboxGroup(
                                        choices=get_extra_options(is_video=False),
                                        value=[],
                                        label="",
                                        info=get_text("extra_options_info")
                                    )

                                with gr.Accordion(get_text("custom_prompt_override"), open=False):
                                    batch_custom_prompt = gr.Textbox(
                                        placeholder=get_text("custom_prompt_placeholder"),
                                        lines=3,
                                        label=""
                                    )

                                gr.Markdown("### ğŸ’¾ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°")
                                batch_output_folder = gr.Textbox(
                                    label=get_text("output_folder"),
                                    placeholder=get_text("output_folder_placeholder"),
                                    value=""
                                )
                                batch_export_formats = gr.CheckboxGroup(
                                    choices=["TXT", "JSON", "CSV"],
                                    value=["TXT"],
                                    label=get_text("export_format")
                                )

                                with gr.Row():
                                    batch_submit_btn = gr.Button(
                                        get_text("process_batch_btn"),
                                        variant="primary",
                                        elem_classes="generate-btn",
                                        scale=3
                                    )
                                    batch_stop_btn = gr.Button(
                                        get_text("stop_btn"),
                                        variant="stop",
                                        scale=1
                                    )

                            with gr.Column(scale=1, elem_classes="card-style"):
                                gr.Markdown("### ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
                                batch_status = gr.Textbox(
                                    label=get_text("status"),
                                    interactive=False,
                                    elem_classes="status-box"
                                )
                                batch_output = gr.Textbox(
                                    label=get_text("results"),
                                    lines=20,
                                    show_copy_button=True
                                )

                                # Download result file
                                batch_download = gr.File(
                                    label=get_text("download_result"),
                                    visible=True
                                )

                                # Console output
                                batch_console_output = gr.Textbox(
                                    label="ğŸ“Ÿ Console Output",
                                    lines=10,
                                    interactive=False,
                                    show_copy_button=True
                                )

                    # BATCH VIDEOS TAB
                    batch_videos_tab = gr.TabItem("ğŸ¬ ĞŸĞ°ĞºĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾")
                    with batch_videos_tab:
                        with gr.Row():
                            with gr.Column(scale=1, elem_classes="card-style"):
                                gr.Markdown("### ğŸ“ Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ„Ğ°Ğ¹Ğ»Ñ‹")
                                batch_videos = gr.File(
                                    file_count="multiple",
                                    label="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                                    file_types=["video"]
                                )

                                gr.Markdown("### ğŸ“ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ")

                                # Prompt preset dropdown
                                with gr.Row():
                                    batch_video_preset = gr.Dropdown(
                                        choices=list(load_prompt_presets().keys()),
                                        value="None",
                                        label=get_text("prompt_preset"),
                                        info=get_text("prompt_preset_info"),
                                        scale=4
                                    )
                                    batch_video_refresh_presets = gr.Button(
                                        "ğŸ”„",
                                        size="sm",
                                        scale=0,
                                        min_width=40
                                    )

                                batch_video_desc_type = gr.Dropdown(
                                    choices=get_description_types(),
                                    value=get_description_types()[0],
                                    label=get_text("description_type"),
                                    info=get_text("description_type_info")
                                )
                                batch_video_desc_length = gr.Dropdown(
                                    choices=get_description_lengths(),
                                    value=get_description_lengths()[0],
                                    label=get_text("description_length"),
                                    info=get_text("description_length_info")
                                )
                                batch_video_num_variants = gr.Slider(
                                    minimum=1,
                                    maximum=3,
                                    value=1,
                                    step=1,
                                    label=get_text("num_variants"),
                                    info=get_text("num_variants_info")
                                )

                                # Character name field
                                batch_video_character_name = gr.Textbox(
                                    label=get_text("character_name"),
                                    placeholder=get_text("character_name_placeholder"),
                                    info=get_text("character_name_info"),
                                    lines=1
                                )

                                # Extra options for videos
                                with gr.Accordion(get_text("extra_options"), open=False):
                                    batch_video_extra_options = gr.CheckboxGroup(
                                        choices=get_extra_options(is_video=True),
                                        value=[],
                                        label="",
                                        info=get_text("extra_options_info")
                                    )

                                with gr.Accordion(get_text("custom_prompt_override"), open=False):
                                    batch_video_custom_prompt = gr.Textbox(
                                        placeholder=get_text("custom_prompt_placeholder"),
                                        lines=3,
                                        label=""
                                    )

                                gr.Markdown("### ğŸ’¾ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°")
                                batch_video_output_folder = gr.Textbox(
                                    label=get_text("output_folder"),
                                    placeholder=get_text("output_folder_placeholder"),
                                    value=""
                                )
                                batch_video_export_formats = gr.CheckboxGroup(
                                    choices=["TXT", "JSON", "CSV"],
                                    value=["TXT"],
                                    label=get_text("export_format")
                                )

                                with gr.Row():
                                    batch_video_submit_btn = gr.Button(
                                        "ğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾",
                                        variant="primary",
                                        elem_classes="generate-btn",
                                        scale=3
                                    )
                                    batch_video_stop_btn = gr.Button(
                                        get_text("stop_btn"),
                                        variant="stop",
                                        scale=1
                                    )

                            with gr.Column(scale=1, elem_classes="card-style"):
                                gr.Markdown("### ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
                                batch_video_status = gr.Textbox(
                                    label=get_text("status"),
                                    interactive=False,
                                    elem_classes="status-box"
                                )
                                batch_video_output = gr.Textbox(
                                    label=get_text("results"),
                                    lines=20,
                                    show_copy_button=True
                                )

                                # Download result file
                                batch_video_download = gr.File(
                                    label=get_text("download_result"),
                                    visible=True
                                )

                                # Console output
                                batch_video_console_output = gr.Textbox(
                                    label="ğŸ“Ÿ Console Output",
                                    lines=10,
                                    interactive=False,
                                    show_copy_button=True
                                )

        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹

        # Function to update extra options based on media type
        def update_extra_options_for_media(is_video):
            return gr.update(choices=get_extra_options(is_video=is_video), value=[])

        # Media tab selection handlers - update extra options when switching tabs
        def on_image_tab_select():
            return gr.update(choices=get_extra_options(is_video=False), value=[]), False, "image"

        def on_video_tab_select():
            return gr.update(choices=get_extra_options(is_video=True), value=[]), True, "video"

        def on_multi_image_tab_select():
            return gr.update(choices=get_extra_options(is_video=False), value=[]), False, "multi"

        # Connect media tab selection to extra options update and active media type
        image_tab.select(fn=on_image_tab_select, outputs=[single_extra_options, single_is_video, active_media_type])
        video_tab.select(fn=on_video_tab_select, outputs=[single_extra_options, single_is_video, active_media_type])
        multi_image_tab.select(fn=on_multi_image_tab_select, outputs=[single_extra_options, single_is_video, active_media_type])

        def change_language(lang):
            global current_language
            current_language = lang

            # Return updated text for key components
            return [
                f"{get_text('subtitle')}",  # header_md
                gr.update(label=get_text("model_selection"), info=get_text("model_info")),  # model_dropdown
                gr.update(label=get_text("quantization"), info=get_text("quantization_info")),  # quantization_dropdown
                gr.update(label=get_text("language"), info=get_text("language_info")),  # language_dropdown
                gr.update(label=get_text("advanced_params")),  # advanced_accordion
                gr.update(value=get_text("generate_btn")),  # single_submit_btn
                gr.update(value=get_text("process_batch_btn")),  # batch_submit_btn
                gr.update(choices=get_description_types(), value=get_description_types()[0]),  # single_desc_type
                gr.update(choices=get_description_lengths(), value=get_description_lengths()[0]),  # single_desc_length
                gr.update(choices=get_description_types(), value=get_description_types()[0]),  # batch_desc_type
                gr.update(choices=get_description_lengths(), value=get_description_lengths()[0]),  # batch_desc_length
                gr.update(choices=get_extra_options(is_video=False), value=[]),  # single_extra_options
                gr.update(choices=get_extra_options(is_video=False), value=[]),  # batch_extra_options
                gr.update(choices=get_extra_options(is_video=True), value=[]),  # batch_video_extra_options
                gr.update(value=get_text("stop_btn")),  # single_stop_btn
                gr.update(value=get_text("stop_btn")),  # batch_stop_btn
                gr.update(value=get_text("stop_btn")),  # batch_video_stop_btn
                gr.update(value=get_text("generate_btn")),  # single_generate_btn_image
                gr.update(value=get_text("stop_btn")),  # single_stop_btn_image
                gr.update(value=get_text("generate_btn")),  # single_generate_btn_video
                gr.update(value=get_text("stop_btn")),  # single_stop_btn_video
            ]

        language_dropdown.change(
            fn=change_language,
            inputs=language_dropdown,
            outputs=[
                header_md,
                model_dropdown,
                quantization_dropdown,
                language_dropdown,
                advanced_accordion,
                single_submit_btn,
                batch_submit_btn,
                single_desc_type,
                single_desc_length,
                batch_desc_type,
                batch_desc_length,
                single_extra_options,
                batch_extra_options,
                batch_video_extra_options,
                single_stop_btn,
                batch_stop_btn,
                batch_video_stop_btn,
                single_generate_btn_image,
                single_stop_btn_image,
                single_generate_btn_video,
                single_stop_btn_video,
            ]
        )

        # Stop button handlers
        single_stop_btn.click(
            fn=stop_generation,
            outputs=single_status
        )

        batch_stop_btn.click(
            fn=stop_generation,
            outputs=batch_status
        )

        batch_video_stop_btn.click(
            fn=stop_generation,
            outputs=batch_video_status
        )

        # Model list refresh handler
        def refresh_model_list():
            """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¾Ğ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸"""
            return gr.update(choices=get_model_choices())

        refresh_models_btn.click(
            fn=refresh_model_list,
            outputs=model_dropdown
        )

        # Preset refresh handlers
        def refresh_presets():
            presets = load_prompt_presets()
            return gr.update(choices=list(presets.keys()), value="None")

        single_refresh_presets.click(
            fn=refresh_presets,
            outputs=single_preset
        )

        batch_refresh_presets.click(
            fn=refresh_presets,
            outputs=batch_preset
        )

        batch_video_refresh_presets.click(
            fn=refresh_presets,
            outputs=batch_video_preset
        )

        # Preset selection handlers (load preset text into custom prompt)
        def load_preset(preset_name):
            presets = load_prompt_presets()
            if preset_name and preset_name != "None":
                return presets.get(preset_name, "")
            return ""

        single_preset.change(
            fn=load_preset,
            inputs=single_preset,
            outputs=single_custom_prompt
        )

        batch_preset.change(
            fn=load_preset,
            inputs=batch_preset,
            outputs=batch_custom_prompt
        )

        batch_video_preset.change(
            fn=load_preset,
            inputs=batch_video_preset,
            outputs=batch_video_custom_prompt
        )

        random_seed_btn.click(
            fn=random_seed,
            outputs=seed_number
        )

        # Update variant visibility based on slider
        def update_variant_visibility(num_variants):
            updates = []
            for i in range(5):
                updates.append(gr.update(visible=(i < num_variants)))
            return updates

        single_num_variants.change(
            fn=update_variant_visibility,
            inputs=[single_num_variants],
            outputs=[group for group, _ in single_outputs]
        )

        # Single image processing with button lock
        def process_single_wrapper(image, video, video_start_time, video_end_time, desc_type, desc_length, custom_prompt,
                                   extra_options, character_name, num_variants,
                                   model_name, quantization, max_tokens, temperature, top_p, top_k, seed, active_media_type):
            # Force media type based on active tab
            if active_media_type == "image":
                video = None  # Ignore video input when image tab is active
            elif active_media_type == "video":
                image = None  # Ignore image input when video tab is active
            elif active_media_type == "multi":
                # Multi-image tab should not use this wrapper
                pass

            # Start capturing console output
            log_capture.clear_logs()
            log_capture.start_capture()

            # Disable all generate buttons, enable all stop buttons
            yield (
                gr.update(value=get_text("generating"), interactive=False),  # single_submit_btn
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_image
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_video
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_multi
                gr.update(interactive=True),   # single_stop_btn
                gr.update(interactive=True),   # single_stop_btn_image
                gr.update(interactive=True),   # single_stop_btn_video
                gr.update(interactive=True),   # single_stop_btn_multi
                "",  # single_status
                gr.update(visible=False),  # thinking_accordion
                "",  # thinking_output
                "",  # single_prompt_used
                gr.update(visible=False),  # image_preview_accordion
                None,  # image_preview
                *[gr.update(value="") for _ in range(5)],  # variant outputs
                None,  # single_download
                ""  # single_console_output
            )

            results = []
            download_path = None

            # Process and yield results
            for status, prompt_used, results, download_path, console_logs, reasoning, image_preview in process_single_image(
                image, video, video_start_time, video_end_time, desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                model_name, quantization, max_tokens, temperature, top_p, top_k, seed
            ):
                # Prepare outputs for each variant box
                variant_outputs = []
                for i in range(5):
                    if i < len(results):
                        variant_outputs.append(gr.update(value=results[i]))
                    else:
                        variant_outputs.append(gr.update(value=""))

                # Add model info above status
                cached_size = get_model_cache_size(model_name)
                size_str = f" [{cached_size}]" if cached_size else ""
                status_with_model = f"âœ… **{model_name}**{size_str} | {quantization}\n\n{status}"

                # Update thinking and image preview visibility
                thinking_visible = bool(reasoning)
                image_preview_visible = bool(image_preview)

                # Keep buttons disabled during generation
                yield (
                    gr.update(value=get_text("generating"), interactive=False),  # single_submit_btn
                    gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_image
                    gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_video
                    gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_multi
                    gr.update(interactive=True),   # single_stop_btn
                    gr.update(interactive=True),   # single_stop_btn_image
                    gr.update(interactive=True),   # single_stop_btn_video
                    gr.update(interactive=True),   # single_stop_btn_multi
                    status_with_model,  # single_status
                    gr.update(visible=thinking_visible),  # thinking_accordion
                    reasoning if reasoning else "",  # thinking_output
                    prompt_used,  # single_prompt_used
                    gr.update(visible=image_preview_visible),  # image_preview_accordion
                    image_preview,  # image_preview
                    *variant_outputs,  # variant outputs
                    download_path,  # single_download
                    console_logs  # single_console_output
                )

            # Stop capturing and get final logs
            log_capture.stop_capture()
            final_logs = log_capture.get_logs()

            # Re-enable button at end
            final_outputs = []
            for i in range(5):
                if i < len(results):
                    final_outputs.append(gr.update(value=results[i]))
                else:
                    final_outputs.append(gr.update(value=""))

            # Add model info above final status
            cached_size = get_model_cache_size(model_name)
            size_str = f" [{cached_size}]" if cached_size else ""
            final_status_with_model = f"âœ… **{model_name}**{size_str} | {quantization}\n\n{status}"

            # Update final thinking and image preview visibility
            final_thinking_visible = bool(reasoning)
            final_image_preview_visible = bool(image_preview)

            # Re-enable all generate buttons, disable all stop buttons
            yield (
                gr.update(value=get_text("generate_btn"), interactive=True),  # single_submit_btn
                gr.update(value=get_text("generate_btn"), interactive=True),  # single_generate_btn_image
                gr.update(value=get_text("generate_btn"), interactive=True),  # single_generate_btn_video
                gr.update(value=get_text("generate_btn"), interactive=True),  # single_generate_btn_multi
                gr.update(interactive=False),  # single_stop_btn
                gr.update(interactive=False),  # single_stop_btn_image
                gr.update(interactive=False),  # single_stop_btn_video
                gr.update(interactive=False),  # single_stop_btn_multi
                final_status_with_model,  # single_status
                gr.update(visible=final_thinking_visible),  # thinking_accordion
                reasoning if reasoning else "",  # thinking_output
                prompt_used,  # single_prompt_used
                gr.update(visible=final_image_preview_visible),  # image_preview_accordion
                image_preview,  # image_preview
                *final_outputs,  # variant outputs
                download_path,  # single_download
                final_logs  # single_console_output
            )

        # Image-only wrapper - ignores video input
        def process_image_only_wrapper(image, desc_type, desc_length, custom_prompt,
                                       extra_options, character_name, num_variants,
                                       model_name, quantization, max_tokens, temperature, top_p, top_k, seed):
            # Force video to None and use default video timestamps
            yield from process_single_wrapper(
                image, None, 0, 7200,  # video=None, default timestamps
                desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                model_name, quantization, max_tokens, temperature, top_p, top_k, seed,
                "image"  # active_media_type
            )

        # Video-only wrapper - ignores image input
        def process_video_only_wrapper(video, video_start_time, video_end_time, desc_type, desc_length, custom_prompt,
                                       extra_options, character_name, num_variants,
                                       model_name, quantization, max_tokens, temperature, top_p, top_k, seed):
            # Force image to None
            yield from process_single_wrapper(
                None, video, video_start_time, video_end_time,  # image=None
                desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                model_name, quantization, max_tokens, temperature, top_p, top_k, seed,
                "video"  # active_media_type
            )

        # Handler for using the prompt from single_prompt_used
        use_prompt_btn.click(
            fn=lambda x: x,  # Just pass through the value
            inputs=[single_prompt_used],
            outputs=[single_custom_prompt]
        )

        single_submit_btn.click(
            fn=process_single_wrapper,
            inputs=[
                single_image,
                single_video,
                video_start_time,
                video_end_time,
                single_desc_type,
                single_desc_length,
                single_custom_prompt,
                single_extra_options,
                single_character_name,
                single_num_variants,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number,
                active_media_type
            ],
            outputs=[single_submit_btn, single_generate_btn_image, single_generate_btn_video, single_generate_btn_multi, single_stop_btn, single_stop_btn_image, single_stop_btn_video, single_stop_btn_multi, single_status, thinking_accordion, thinking_output, single_prompt_used, image_preview_accordion, image_preview] + [output for _, output in single_outputs] + [single_download, single_console_output]
        )

        # Duplicate Generate buttons in Image/Video tabs - use specific wrappers
        single_generate_btn_image.click(
            fn=process_image_only_wrapper,
            inputs=[
                single_image,
                # video inputs removed - image tab only uses image
                single_desc_type,
                single_desc_length,
                single_custom_prompt,
                single_extra_options,
                single_character_name,
                single_num_variants,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=[single_submit_btn, single_generate_btn_image, single_generate_btn_video, single_generate_btn_multi, single_stop_btn, single_stop_btn_image, single_stop_btn_video, single_stop_btn_multi, single_status, thinking_accordion, thinking_output, single_prompt_used, image_preview_accordion, image_preview] + [output for _, output in single_outputs] + [single_download, single_console_output]
        )

        single_generate_btn_video.click(
            fn=process_video_only_wrapper,
            inputs=[
                single_video,
                video_start_time,
                video_end_time,
                # image input removed - video tab only uses video
                single_desc_type,
                single_desc_length,
                single_custom_prompt,
                single_extra_options,
                single_character_name,
                single_num_variants,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=[single_submit_btn, single_generate_btn_image, single_generate_btn_video, single_generate_btn_multi, single_stop_btn, single_stop_btn_image, single_stop_btn_video, single_stop_btn_multi, single_status, thinking_accordion, thinking_output, single_prompt_used, image_preview_accordion, image_preview] + [output for _, output in single_outputs] + [single_download, single_console_output]
        )

        # Duplicate Stop buttons in Image/Video tabs
        single_stop_btn_image.click(
            fn=stop_generation,
            outputs=single_status
        )

        single_stop_btn_video.click(
            fn=stop_generation,
            outputs=single_status
        )

        # Multi-image processing wrapper
        def process_multi_image_wrapper(multi_images_list, desc_type, desc_length, custom_prompt,
                                       extra_options, character_name, num_variants,
                                       model_name, quantization, max_tokens, temperature, top_p, top_k, seed):
            # Start capturing console output
            log_capture.clear_logs()
            log_capture.start_capture()

            # Disable all generate buttons, enable all stop buttons
            yield (
                gr.update(value=get_text("generating"), interactive=False),  # single_submit_btn
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_image
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_video
                gr.update(value=get_text("generating"), interactive=False),  # single_generate_btn_multi
                gr.update(interactive=True),   # single_stop_btn
                gr.update(interactive=True),   # single_stop_btn_image
                gr.update(interactive=True),   # single_stop_btn_video
                gr.update(interactive=True),   # single_stop_btn_multi
                "",  # single_status
                gr.update(visible=False),  # thinking_accordion
                "",  # thinking_output
                "",  # single_prompt_used
                gr.update(visible=False),  # image_preview_accordion
                None,  # image_preview
                *[gr.update(value="") for _ in range(5)],  # variant outputs
                None,  # single_download
                ""  # single_console_output
            )

            results = []
            download_path = None

            # Process and yield results
            for status, prompt_used, results, download_path, console_logs in process_multi_image(
                multi_images_list, desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                model_name, quantization, max_tokens, temperature, top_p, top_k, seed
            ):
                # Prepare outputs for each variant box
                variant_outputs = []
                for i in range(5):
                    if i < len(results):
                        variant_outputs.append(gr.update(value=results[i]))
                    else:
                        variant_outputs.append(gr.update(value=""))

                # Add model info above status
                cached_size = get_model_cache_size(model_name)
                size_str = f" [{cached_size}]" if cached_size else ""
                status_with_model = f"âœ… **{model_name}**{size_str} | {quantization}\n\n{status}"

                # Keep buttons disabled during generation
                yield (
                    gr.update(value=get_text("generating"), interactive=False),
                    gr.update(value=get_text("generating"), interactive=False),
                    gr.update(value=get_text("generating"), interactive=False),
                    gr.update(value=get_text("generating"), interactive=False),
                    gr.update(interactive=True),
                    gr.update(interactive=True),
                    gr.update(interactive=True),
                    gr.update(interactive=True),
                    status_with_model,  # single_status
                    gr.update(visible=False),  # thinking_accordion
                    "",  # thinking_output
                    prompt_used,  # single_prompt_used
                    gr.update(visible=False),  # image_preview_accordion
                    None,  # image_preview
                    *variant_outputs,  # variant outputs
                    download_path,  # single_download
                    console_logs  # single_console_output
                )

            # Stop capturing and get final logs
            log_capture.stop_capture()
            final_logs = log_capture.get_logs()

            # Re-enable button at end
            final_outputs = []
            for i in range(5):
                if i < len(results):
                    final_outputs.append(gr.update(value=results[i]))
                else:
                    final_outputs.append(gr.update(value=""))

            # Add model info above final status
            cached_size = get_model_cache_size(model_name)
            size_str = f" [{cached_size}]" if cached_size else ""
            final_status_with_model = f"âœ… **{model_name}**{size_str} | {quantization}\n\n{status}"

            # Re-enable all generate buttons, disable all stop buttons
            yield (
                gr.update(value=get_text("generate_btn"), interactive=True),
                gr.update(value=get_text("generate_btn"), interactive=True),
                gr.update(value=get_text("generate_btn"), interactive=True),
                gr.update(value=get_text("generate_btn"), interactive=True),
                gr.update(interactive=False),
                gr.update(interactive=False),
                gr.update(interactive=False),
                gr.update(interactive=False),
                final_status_with_model,  # single_status
                gr.update(visible=False),  # thinking_accordion
                "",  # thinking_output
                prompt_used,  # single_prompt_used
                gr.update(visible=False),  # image_preview_accordion
                None,  # image_preview
                *final_outputs,  # variant outputs
                download_path,  # single_download
                final_logs  # single_console_output
            )

        # Wire up multi-image generate button
        single_generate_btn_multi.click(
            fn=process_multi_image_wrapper,
            inputs=[
                multi_images,
                single_desc_type,
                single_desc_length,
                single_custom_prompt,
                single_extra_options,
                single_character_name,
                single_num_variants,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=[single_submit_btn, single_generate_btn_image, single_generate_btn_video, single_generate_btn_multi,
                     single_stop_btn, single_stop_btn_image, single_stop_btn_video, single_stop_btn_multi,
                     single_status, thinking_accordion, thinking_output, single_prompt_used, image_preview_accordion, image_preview] + [output for _, output in single_outputs] + [single_download, single_console_output]
        )

        # Wire up multi-image stop button
        single_stop_btn_multi.click(
            fn=stop_generation,
            outputs=single_status
        )

        # Update video sliders when video is loaded
        single_video.change(
            fn=update_video_sliders,
            inputs=[single_video],
            outputs=[video_start_time, video_end_time]
        )

        # Save preset handler
        def save_preset(name, prompt):
            if not name:
                return "âŒ Please enter a preset name"
            msg = save_prompt_preset(name, prompt)
            new_presets = list(load_prompt_presets().keys())
            return msg, gr.update(choices=new_presets, value=name if "successfully" in msg else None)

        single_save_preset_btn.click(
            fn=save_preset,
            inputs=[single_save_preset_name, single_custom_prompt],
            outputs=[single_save_preset_status, single_preset]
        )

        # Batch processing with button lock
        def process_batch_wrapper(files, desc_type, desc_length, custom_prompt,
                                  extra_options, character_name, num_variants,
                                  output_folder, export_formats, model_name, quantization,
                                  max_tokens, temperature, top_p, top_k, seed):
            # Start capturing console output
            log_capture.clear_logs()
            log_capture.start_capture()

            # Disable button at start
            yield gr.update(value=get_text("generating"), interactive=False), "", "", None, ""

            download_path = None

            # Process and yield results
            for status, output_text, download_path in process_batch_images(
                files, desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                output_folder, export_formats, model_name, quantization,
                max_tokens, temperature, top_p, top_k, seed
            ):
                yield gr.update(value=get_text("generating"), interactive=False), status, output_text, download_path, log_capture.get_logs()

            # Stop capturing and get final logs
            log_capture.stop_capture()
            final_logs = log_capture.get_logs()

            # Re-enable button at end
            yield gr.update(value=get_text("process_batch_btn"), interactive=True), status, output_text, download_path, final_logs

        batch_submit_btn.click(
            fn=process_batch_wrapper,
            inputs=[
                batch_images,
                batch_desc_type,
                batch_desc_length,
                batch_custom_prompt,
                batch_extra_options,
                batch_character_name,
                batch_num_variants,
                batch_output_folder,
                batch_export_formats,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=[batch_submit_btn, batch_status, batch_output, batch_download, batch_console_output]
        )

        # Batch video processing with is_video=True
        def process_batch_video_wrapper(files, desc_type, desc_length, custom_prompt,
                                        extra_options, character_name, num_variants,
                                        output_folder, export_formats, model_name, quantization,
                                        max_tokens, temperature, top_p, top_k, seed):
            # Start capturing console output
            log_capture.clear_logs()
            log_capture.start_capture()

            # Disable button at start
            yield gr.update(value="â³ Processing...", interactive=False), "", "", None, ""

            download_path = None

            # Process and yield results with is_video=True
            for status, output_text, download_path in process_batch_images(
                files, desc_type, desc_length, custom_prompt,
                extra_options, character_name, num_variants,
                output_folder, export_formats, model_name, quantization,
                max_tokens, temperature, top_p, top_k, seed,
                is_video=True  # KEY DIFFERENCE: process as videos
            ):
                yield gr.update(value="â³ Processing...", interactive=False), status, output_text, download_path, log_capture.get_logs()

            # Stop capturing and get final logs
            log_capture.stop_capture()
            final_logs = log_capture.get_logs()

            # Re-enable button at end
            yield gr.update(value="ğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾", interactive=True), status, output_text, download_path, final_logs

        batch_video_submit_btn.click(
            fn=process_batch_video_wrapper,
            inputs=[
                batch_videos,
                batch_video_desc_type,
                batch_video_desc_length,
                batch_video_custom_prompt,
                batch_video_extra_options,
                batch_video_character_name,
                batch_video_num_variants,
                batch_video_output_folder,
                batch_video_export_formats,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=[batch_video_submit_btn, batch_video_status, batch_video_output, batch_video_download, batch_video_console_output]
        )

        return demo

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Gradio
demo = create_interface()

if __name__ == "__main__":
    # Enable queue for progress bar support
    demo.queue(max_size=20, default_concurrency_limit=1).launch(
        server_name="127.0.0.1",
        server_port=None,
        share=False,
        show_error=True,
        inbrowser=True
    )
