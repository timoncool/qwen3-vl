import gradio as gr
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
from PIL import Image
import random
import os
import warnings
from typing import List, Tuple, Optional
import requests
from io import BytesIO
import urllib.parse
import gc

# Suppress specific warnings
warnings.filterwarnings('ignore', message='.*meta device.*')

# Base directory for portable app
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRIPT_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output")

# Create directories if they don't exist
os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Multi-language support
TRANSLATIONS = {
    "en": {
        "title": "Qwen VL Image Description Generator",
        "header": "üñºÔ∏è Image Description Generator based on Qwen Vision Language Models",
        "subtitle": "Upload an image and enter a prompt to generate a description using Qwen VL models.",
        "language": "Language",
        "language_info": "Select language",
        "model_selection": "Model Selection",
        "model_info": "Select a model for generating descriptions",
        "advanced_params": "‚öôÔ∏è Advanced Parameters",
        "max_tokens": "Max New Tokens",
        "max_tokens_info": "Maximum number of tokens to generate",
        "temperature": "Temperature",
        "temperature_info": "Controls randomness of generation",
        "top_p": "Top-p (nucleus sampling)",
        "top_p_info": "Probability threshold for token sampling",
        "top_k": "Top-k",
        "top_k_info": "Number of most probable tokens to consider",
        "seed": "Seed",
        "seed_info": "Seed for reproducibility (-1 for random)",
        "random_seed_btn": "üé≤ Random Seed",
        "single_processing": "üìÑ Single Processing",
        "batch_processing": "üìö Batch Processing",
        "upload_image": "Upload Image",
        "image_url": "Or enter Image URL",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "Prompt",
        "prompt_placeholder": "For example: Create a product description for online store",
        "generate_btn": "üöÄ Generate Description",
        "result": "Result",
        "upload_images": "Upload Images",
        "prompts_multiline": "Prompts (one per line)",
        "prompts_placeholder": "Create a product description for online store\nCreate SEO Description for product\n...",
        "prompts_info": "Specify one prompt for all images or one prompt per image",
        "process_batch_btn": "üöÄ Process Batch",
        "results": "Results",
        "examples_title": "üí° Example Prompts:",
        "example_1": "Create a product description for online store",
        "example_2": "Create an SEO description for a product with a maximum of 160 characters.",
        "example_3": "Create an attractive product description for marketplace",
        "example_4": "Describe image in detail for product catalog",
        "error_no_image": "Please upload an image or provide an image URL",
        "error_no_prompt": "Please enter a prompt",
        "error_no_images": "Please upload images",
        "error_no_prompts": "Please enter prompts (one per line)",
        "error_prompt_mismatch": "Number of prompts ({}) does not match number of images ({}). Specify either one prompt for all images or one prompt per image.",
        "error_generation": "Error generating description: {}",
        "error_url_load": "Error loading image from URL: {}",
        "loading_model": "Loading model: {}",
        "model_loaded": "Model {} successfully loaded on {}",
        "image_label": "=== Image {}: {} ===",
        "prompt_label": "Prompt: {}",
        "result_label": "Result: {}",
        "model_size_warning": "‚ö†Ô∏è Note: Large models (8B+) may use CPU offloading if GPU memory is insufficient, which can slow down generation.",
        "quantization": "Quantization",
        "quantization_info": "Memory optimization (4-bit = ~75% less VRAM)",
        "quant_4bit": "4-bit (Recommended)",
        "quant_8bit": "8-bit (Better quality)",
        "quant_none": "None (Full precision)"
    },
    "ru": {
        "title": "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π Qwen VL",
        "header": "üñºÔ∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen Vision Language Models",
        "subtitle": "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π Qwen VL.",
        "language": "–Ø–∑—ã–∫",
        "language_info": "–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫",
        "model_selection": "–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏",
        "model_info": "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π",
        "advanced_params": "‚öôÔ∏è –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
        "max_tokens": "–ú–∞–∫—Å. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤",
        "max_tokens_info": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
        "temperature": "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
        "temperature_info": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
        "top_p": "Top-p (nucleus sampling)",
        "top_p_info": "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤",
        "top_k": "Top-k",
        "top_k_info": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è",
        "seed": "Seed",
        "seed_info": "Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ (-1 –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ)",
        "random_seed_btn": "üé≤ –°–ª—É—á–∞–π–Ω—ã–π seed",
        "single_processing": "üìÑ –û–¥–∏–Ω–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞",
        "batch_processing": "üìö –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞",
        "upload_image": "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        "image_url": "–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "–ü—Ä–æ–º—Ç",
        "prompt_placeholder": "–ù–∞–ø—Ä–∏–º–µ—Ä: –°–æ–∑–¥–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –æ–Ω–ª–∞–π–Ω –º–∞–≥–∞–∑–∏–Ω–∞",
        "generate_btn": "üöÄ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ",
        "result": "–†–µ–∑—É–ª—å—Ç–∞—Ç",
        "upload_images": "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        "prompts_multiline": "–ü—Ä–æ–º—Ç—ã (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
        "prompts_placeholder": "–°–æ–∑–¥–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –æ–Ω–ª–∞–π–Ω –º–∞–≥–∞–∑–∏–Ω–∞\n–°–æ–∑–¥–∞—Ç—å SEO Description –¥–ª—è —Ç–æ–≤–∞—Ä–∞\n...",
        "prompts_info": "–£–∫–∞–∂–∏—Ç–µ –æ–¥–∏–Ω –ø—Ä–æ–º—Ç –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–æ–º—Ç—É –Ω–∞ –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        "process_batch_btn": "üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞–∫–µ—Ç",
        "results": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã",
        "examples_title": "üí° –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º—Ç–æ–≤:",
        "example_1": "–°–æ–∑–¥–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ ''  –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
        "example_2": "–°–æ–∑–¥–∞—Ç—å SEO Description –¥–ª—è —Ç–æ–≤–∞—Ä–∞ –º–∞–∫—Å–∏–º—É–º 160 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
        "example_3": "–°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –¥–ª—è –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
        "example_4": "–î–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—Å–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
        "error_no_image": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        "error_no_prompt": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º—Ç",
        "error_no_images": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        "error_no_prompts": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º—Ç—ã (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
        "error_prompt_mismatch": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º—Ç–æ–≤ ({}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({}). –£–∫–∞–∂–∏—Ç–µ –ª–∏–±–æ –æ–¥–∏–Ω –ø—Ä–æ–º—Ç –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ª–∏–±–æ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–æ–º—Ç—É –Ω–∞ –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.",
        "error_generation": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {}",
        "error_url_load": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL: {}",
        "loading_model": "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {}",
        "model_loaded": "–ú–æ–¥–µ–ª—å {} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {}",
        "image_label": "=== –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {}: {} ===",
        "prompt_label": "–ü—Ä–æ–º—Ç: {}",
        "result_label": "–†–µ–∑—É–ª—å—Ç–∞—Ç: {}",
        "model_size_warning": "‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (8B+) –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã–≥—Ä—É–∑–∫—É –Ω–∞ CPU –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏ GPU, —á—Ç–æ –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.",
        "quantization": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è",
        "quantization_info": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ (4-bit = ~75% –º–µ–Ω—å—à–µ VRAM)",
        "quant_4bit": "4-bit (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)",
        "quant_8bit": "8-bit (–õ—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)",
        "quant_none": "–ù–µ—Ç (–ü–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)"
    },
    "zh": {
        "title": "Qwen VL ÂõæÂÉèÊèèËø∞ÁîüÊàêÂô®",
        "header": "üñºÔ∏è Âü∫‰∫é Qwen Vision Language Models ÁöÑÂõæÂÉèÊèèËø∞ÁîüÊàêÂô®",
        "subtitle": "‰∏ä‰º†ÂõæÂÉèÂπ∂ËæìÂÖ•ÊèêÁ§∫ËØçÔºå‰ΩøÁî® Qwen VL Ê®°ÂûãÁîüÊàêÊèèËø∞„ÄÇ",
        "language": "ËØ≠Ë®Ä",
        "language_info": "ÈÄâÊã©ËØ≠Ë®Ä",
        "model_selection": "Ê®°ÂûãÈÄâÊã©",
        "model_info": "ÈÄâÊã©Áî®‰∫éÁîüÊàêÊèèËø∞ÁöÑÊ®°Âûã",
        "advanced_params": "‚öôÔ∏è È´òÁ∫ßÂèÇÊï∞",
        "max_tokens": "ÊúÄÂ§ßÊñ∞‰ª§ÁâåÊï∞",
        "max_tokens_info": "ÁîüÊàêÁöÑÊúÄÂ§ß‰ª§ÁâåÊï∞",
        "temperature": "Ê∏©Â∫¶",
        "temperature_info": "ÊéßÂà∂ÁîüÊàêÁöÑÈöèÊú∫ÊÄß",
        "top_p": "Top-pÔºàÊ†∏ÈááÊ†∑Ôºâ",
        "top_p_info": "‰ª§ÁâåÈááÊ†∑ÁöÑÊ¶ÇÁéáÈòàÂÄº",
        "top_k": "Top-k",
        "top_k_info": "ËÄÉËôëÁöÑÊúÄÂèØËÉΩ‰ª§ÁâåÊï∞",
        "seed": "ÈöèÊú∫ÁßçÂ≠ê",
        "seed_info": "Áî®‰∫éÂèØÈáçÁé∞ÊÄßÁöÑÁßçÂ≠êÔºà-1 Ë°®Á§∫ÈöèÊú∫Ôºâ",
        "random_seed_btn": "üé≤ ÈöèÊú∫ÁßçÂ≠ê",
        "single_processing": "üìÑ ÂçïÂº†Â§ÑÁêÜ",
        "batch_processing": "üìö ÊâπÈáèÂ§ÑÁêÜ",
        "upload_image": "‰∏ä‰º†ÂõæÂÉè",
        "image_url": "ÊàñËæìÂÖ•ÂõæÂÉèURL",
        "image_url_placeholder": "https://example.com/image.jpg",
        "prompt": "ÊèêÁ§∫ËØç",
        "prompt_placeholder": "‰æãÂ¶ÇÔºö‰∏∫Âú®Á∫øÂïÜÂ∫óÂàõÂª∫‰∫ßÂìÅÊèèËø∞",
        "generate_btn": "üöÄ ÁîüÊàêÊèèËø∞",
        "result": "ÁªìÊûú",
        "upload_images": "‰∏ä‰º†ÂõæÂÉè",
        "prompts_multiline": "ÊèêÁ§∫ËØçÔºàÊØèË°å‰∏Ä‰∏™Ôºâ",
        "prompts_placeholder": "‰∏∫Âú®Á∫øÂïÜÂ∫óÂàõÂª∫‰∫ßÂìÅÊèèËø∞\n‰∏∫‰∫ßÂìÅÂàõÂª∫SEOÊèèËø∞\n...",
        "prompts_info": "‰∏∫ÊâÄÊúâÂõæÂÉèÊåáÂÆö‰∏Ä‰∏™ÊèêÁ§∫ËØçÔºåÊàñ‰∏∫ÊØè‰∏™ÂõæÂÉèÊåáÂÆö‰∏Ä‰∏™ÊèêÁ§∫ËØç",
        "process_batch_btn": "üöÄ Â§ÑÁêÜÊâπÊ¨°",
        "results": "ÁªìÊûú",
        "examples_title": "üí° Á§∫‰æãÊèêÁ§∫ËØçÔºö",
        "example_1": "‰∏∫Âú®Á∫øÂïÜÂ∫óÂàõÂª∫‰∫ßÂìÅÊèèËø∞",
        "example_2": "‰∏∫‰∫ßÂìÅÂàõÂª∫SEOÊèèËø∞ÊúÄÂ§ö 160 ‰∏™Â≠óÁ¨¶",
        "example_3": "‰∏∫Â∏ÇÂú∫ÂàõÂª∫ÊúâÂê∏ÂºïÂäõÁöÑ‰∫ßÂìÅÊèèËø∞",
        "example_4": "ËØ¶ÁªÜÊèèËø∞‰∫ßÂìÅÁõÆÂΩïÁöÑÂõæÂÉè",
        "error_no_image": "ËØ∑‰∏ä‰º†ÂõæÂÉèÊàñÊèê‰æõÂõæÂÉèURL",
        "error_no_prompt": "ËØ∑ËæìÂÖ•ÊèêÁ§∫ËØç",
        "error_no_images": "ËØ∑‰∏ä‰º†ÂõæÂÉè",
        "error_no_prompts": "ËØ∑ËæìÂÖ•ÊèêÁ§∫ËØçÔºàÊØèË°å‰∏Ä‰∏™Ôºâ",
        "error_prompt_mismatch": "ÊèêÁ§∫ËØçÊï∞ÈáèÔºà{}Ôºâ‰∏éÂõæÂÉèÊï∞ÈáèÔºà{}Ôºâ‰∏çÂåπÈÖç„ÄÇËØ∑‰∏∫ÊâÄÊúâÂõæÂÉèÊåáÂÆö‰∏Ä‰∏™ÊèêÁ§∫ËØçÔºåÊàñ‰∏∫ÊØè‰∏™ÂõæÂÉèÊåáÂÆö‰∏Ä‰∏™ÊèêÁ§∫ËØç„ÄÇ",
        "error_generation": "ÁîüÊàêÊèèËø∞Êó∂Âá∫ÈîôÔºö{}",
        "error_url_load": "‰ªéURLÂä†ËΩΩÂõæÂÉèÊó∂Âá∫ÈîôÔºö{}",
        "loading_model": "Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÔºö{}",
        "model_loaded": "Ê®°Âûã {} Â∑≤ÊàêÂäüÂä†ËΩΩÂà∞ {}",
        "image_label": "=== ÂõæÂÉè {}: {} ===",
        "prompt_label": "ÊèêÁ§∫ËØçÔºö{}",
        "result_label": "ÁªìÊûúÔºö{}",
        "model_size_warning": "‚ö†Ô∏è Ê≥®ÊÑèÔºöÂ¶ÇÊûú GPU ÂÜÖÂ≠ò‰∏çË∂≥ÔºåÂ§ßÂûãÊ®°ÂûãÔºà8B+ÔºâÂèØËÉΩ‰ºö‰ΩøÁî® CPU Âç∏ËΩΩÔºåËøôÂèØËÉΩ‰ºöÂáèÊÖ¢ÁîüÊàêÈÄüÂ∫¶„ÄÇ",
        "quantization": "ÈáèÂåñ",
        "quantization_info": "ÂÜÖÂ≠ò‰ºòÂåñÔºà4‰Ωç = ÂáèÂ∞ëÁ∫¶75%ÊòæÂ≠òÔºâ",
        "quant_4bit": "4‰ΩçÔºàÊé®ËçêÔºâ",
        "quant_8bit": "8‰ΩçÔºàÊõ¥È´òË¥®ÈáèÔºâ",
        "quant_none": "Êó†ÔºàÂÖ®Á≤æÂ∫¶Ôºâ"
    }
}

# Default language
current_language = "en"

def get_text(key: str) -> str:
    """Get translated text for the current language"""
    return TRANSLATIONS[current_language].get(key, key)

class ImageDescriptionGenerator:
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_name = None
        self.current_quantization = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self, model_name: str, quantization: str = "4-bit"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π BitsAndBytes"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–∞ –ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞
        if (self.current_model_name == model_name and
            self.current_quantization == quantization and
            self.model is not None):
            return

        print(get_text("loading_model").format(model_name))
        print(f"Quantization: {quantization}")

        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö
        if "8B" in model_name or "32B" in model_name or "30B" in model_name:
            print(get_text("model_size_warning"))

        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏
        if self.model is not None:
            del self.model
            del self.processor
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ BitsAndBytes
        bnb_config = None
        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

        if quantization == "4-bit" and torch.cuda.is_available():
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
            print("Using 4-bit quantization (NF4 + double quant) - ~75% VRAM reduction")
        elif quantization == "8-bit" and torch.cuda.is_available():
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
            )
            print("Using 8-bit quantization - ~50% VRAM reduction")
        else:
            print("Using full precision (bfloat16/float32)")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore')
            load_kwargs = {
                "device_map": "auto",
                "trust_remote_code": True,
            }

            if bnb_config is not None:
                load_kwargs["quantization_config"] = bnb_config
            else:
                load_kwargs["torch_dtype"] = dtype

            # SDPA (Scaled Dot Product Attention) - –≤—Å—Ç—Ä–æ–µ–Ω –≤ PyTorch 2.0+
            # –ë—ã—Å—Ç—Ä—ã–π –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ Windows –±–µ–∑ –¥–æ–ø. –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            if torch.cuda.is_available():
                load_kwargs["attn_implementation"] = "sdpa"
                print("Using SDPA attention (PyTorch native, fast)")

            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                **load_kwargs
            )
            self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

        self.current_model_name = model_name
        self.current_quantization = quantization
        print(get_text("model_loaded").format(model_name, self.device))
    
    def generate_description(
        self,
        image_path: str,
        prompt: str,
        model_name: str,
        quantization: str = "4-bit",
        max_new_tokens: int = 1024,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 50,
        seed: int = -1
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            self.load_model(model_name, quantization)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if seed != -1:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": image_path,
                        },
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (inference_mode –±—ã—Å—Ç—Ä–µ–µ —á–µ–º no_grad)
            with torch.inference_mode():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=True if temperature > 0 else False,
                    use_cache=True,  # KV –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            return output_text[0]
            
        except Exception as e:
            return get_text("error_generation").format(str(e))

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
generator = ImageDescriptionGenerator()

def load_image_from_url(url: str) -> Optional[str]:
    """Load image from URL and save to temporary file"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # Load image from response content
        image = Image.open(BytesIO(response.content))
        
        # Save to temporary file
        temp_path = os.path.join(TEMP_DIR, "temp_url_image.jpg")
        image.save(temp_path)
        
        return temp_path
    except Exception as e:
        raise Exception(get_text("error_url_load").format(str(e)))

def process_single_image(
    image,
    image_url: str,
    prompt: str,
    model_name: str,
    quantization: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    seed: int
) -> str:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    # Check if we have either uploaded image or URL
    if image is None and not image_url.strip():
        return get_text("error_no_image")

    if not prompt.strip():
        return get_text("error_no_prompt")

    temp_path = None

    try:
        # Priority: URL over uploaded image (if both provided, URL takes precedence)
        if image_url.strip():
            image_path = load_image_from_url(image_url.strip())
            temp_path = image_path
        elif hasattr(image, 'shape'):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ —ç—Ç–æ numpy array
            temp_path = os.path.join(TEMP_DIR, "temp_image.jpg")
            Image.fromarray(image).save(temp_path)
            image_path = temp_path
        else:
            image_path = image

        result = generator.generate_description(
            image_path=image_path,
            prompt=prompt,
            model_name=model_name,
            quantization=quantization,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            seed=seed
        )

        return result

    except Exception as e:
        return str(e)
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass

def process_batch_images(
    files: List,
    prompts_text: str,
    model_name: str,
    quantization: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    seed: int
) -> str:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    if not files:
        return get_text("error_no_images")

    if not prompts_text.strip():
        return get_text("error_no_prompts")

    # –†–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–æ–º—Ç—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º
    prompts = [p.strip() for p in prompts_text.split('\n') if p.strip()]

    if len(prompts) == 1:
        # –ï—Å–ª–∏ –æ–¥–∏–Ω –ø—Ä–æ–º—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        prompts = prompts * len(files)
    elif len(prompts) != len(files):
        return get_text("error_prompt_mismatch").format(len(prompts), len(files))

    results = []
    for idx, (file, prompt) in enumerate(zip(files, prompts), 1):
        image_path = file.name if hasattr(file, 'name') else file
        result = generator.generate_description(
            image_path=image_path,
            prompt=prompt,
            model_name=model_name,
            quantization=quantization,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            seed=seed if seed == -1 else seed + idx - 1  # –†–∞–∑–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        )
        results.append(get_text("image_label").format(idx, os.path.basename(image_path)) + "\n")
        results.append(get_text("prompt_label").format(prompt) + "\n")
        results.append(get_text("result_label").format(result) + "\n\n")

    return "".join(results)

def random_seed() -> int:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ seed"""
    return random.randint(0, 2**32 - 1)

def update_examples():
    return [
        [get_text("example_1")],
        [get_text("example_2")],
        [get_text("example_3")],
        [get_text("example_4")]
    ]

def create_interface():
    """Create Gradio interface with current language"""
    with gr.Blocks(title=get_text("title"), theme=gr.themes.Soft()) as demo:
        # Header that will be updated
        header_md = gr.Markdown(f"""
        # {get_text("header")}
        
        {get_text("subtitle")}
        """)
        
        # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - –º–æ–¥–µ–ª—å, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –∏ —è–∑—ã–∫
        with gr.Row():
            model_dropdown = gr.Dropdown(
                choices=[
                    # Abliterated models (–±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã) - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ
                    ("2B Instruct Abliterated (~1.2GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated"),
                    ("2B Thinking Abliterated (~1.2GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated"),
                    ("4B Instruct Abliterated (~2.5GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated"),
                    ("4B Thinking Abliterated (~2.5GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated"),
                    ("8B Instruct Abliterated (~5GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"),
                    ("8B Thinking Abliterated (~5GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated"),
                    ("30B-A3B MoE Instruct Abliterated (~18GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated"),
                    ("30B-A3B MoE Thinking Abliterated (~18GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-30B-A3B-Thinking-abliterated"),
                    ("32B Instruct Abliterated (~18GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated"),
                    ("32B Thinking Abliterated (~18GB 4-bit)", "huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated"),
                    # Original Qwen models
                    ("Qwen 2B Instruct (~1.2GB 4-bit)", "Qwen/Qwen3-VL-2B-Instruct"),
                    ("Qwen 4B Instruct (~2.5GB 4-bit)", "Qwen/Qwen3-VL-4B-Instruct"),
                    ("Qwen 8B Instruct (~5GB 4-bit)", "Qwen/Qwen3-VL-8B-Instruct"),
                ],
                value="huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated",
                label=get_text("model_selection"),
                info=get_text("model_info"),
                scale=3
            )
            quantization_dropdown = gr.Dropdown(
                choices=[
                    ("4-bit (Recommended)", "4-bit"),
                    ("8-bit (Better quality)", "8-bit"),
                    ("None (Full precision)", "none"),
                ],
                value="4-bit",
                label=get_text("quantization"),
                info=get_text("quantization_info"),
                scale=1
            )
            language_dropdown = gr.Dropdown(
                choices=[("English", "en"), ("–†—É—Å—Å–∫–∏–π", "ru"), ("‰∏≠Êñá", "zh")],
                value=current_language,
                label=get_text("language"),
                info=get_text("language_info"),
                scale=1
            )
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        advanced_accordion = gr.Accordion(get_text("advanced_params"), open=False)
        with advanced_accordion:
            with gr.Row():
                max_tokens_slider = gr.Slider(
                    minimum=1,
                    maximum=4096,
                    value=1024,
                    step=1,
                    label=get_text("max_tokens"),
                    info=get_text("max_tokens_info")
                )
                temperature_slider = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.6,
                    step=0.1,
                    label=get_text("temperature"),
                    info=get_text("temperature_info")
                )
            
            with gr.Row():
                top_p_slider = gr.Slider(
                    minimum=0.05,
                    maximum=1.0,
                    value=0.9,
                    step=0.05,
                    label=get_text("top_p"),
                    info=get_text("top_p_info")
                )
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=1000,
                    value=50,
                    step=1,
                    label=get_text("top_k"),
                    info=get_text("top_k_info")
                )
            
            with gr.Row():
                seed_number = gr.Number(
                    value=-1,
                    label=get_text("seed"),
                    info=get_text("seed_info"),
                    precision=0
                )
                random_seed_btn = gr.Button(get_text("random_seed_btn"), size="sm")
        
        # –í–∫–ª–∞–¥–∫–∏ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–π –∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tabs = gr.Tabs()
        with tabs:
            # –í–∫–ª–∞–¥–∫–∞ –æ–¥–∏–Ω–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            single_tab = gr.TabItem(get_text("single_processing"))
            with single_tab:
                with gr.Row():
                    with gr.Column(scale=1):
                        single_image = gr.Image(
                            type="numpy",
                            label=get_text("upload_image"),
                            height=350
                        )
                        single_image_url = gr.Textbox(
                            label=get_text("image_url"),
                            placeholder=get_text("image_url_placeholder"),
                            lines=1
                        )
                        single_prompt = gr.Textbox(
                            label=get_text("prompt"),
                            placeholder=get_text("prompt_placeholder"),
                            lines=3
                        )
                        single_submit_btn = gr.Button(get_text("generate_btn"), variant="primary")
                    
                    with gr.Column(scale=1):
                        single_output = gr.Textbox(
                            label=get_text("result"),
                            lines=15,
                            show_copy_button=True
                        )
                
                # –ö–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º—Ç–æ–≤
                examples_title = gr.Markdown(f"### {get_text('examples_title')}")
                single_examples = gr.Dataset(
                    components=[single_prompt],
                    samples=update_examples(),
                    type="values"
                )
            
            # –í–∫–ª–∞–¥–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            batch_tab = gr.TabItem(get_text("batch_processing"))
            with batch_tab:
                with gr.Row():
                    with gr.Column(scale=1):
                        batch_images = gr.File(
                            file_count="multiple",
                            label=get_text("upload_images"),
                            file_types=["image"]
                        )
                        batch_prompts = gr.Textbox(
                            label=get_text("prompts_multiline"),
                            placeholder=get_text("prompts_placeholder"),
                            lines=5,
                            info=get_text("prompts_info")
                        )
                        batch_submit_btn = gr.Button(get_text("process_batch_btn"), variant="primary")
                    
                    with gr.Column(scale=1):
                        batch_output = gr.Textbox(
                            label=get_text("results"),
                            lines=20,
                            show_copy_button=True
                        )
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
        def change_language(lang):
            global current_language
            current_language = lang

            # Return updated text for all components
            return [
                f"""
        # {get_text("header")}

        {get_text("subtitle")}
        """,  # header_md
                gr.update(label=get_text("model_selection"), info=get_text("model_info")),  # model_dropdown
                gr.update(label=get_text("quantization"), info=get_text("quantization_info")),  # quantization_dropdown
                gr.update(label=get_text("language"), info=get_text("language_info")),  # language_dropdown
                gr.update(label=get_text("advanced_params")),  # advanced_accordion
                gr.update(label=get_text("max_tokens"), info=get_text("max_tokens_info")),  # max_tokens_slider
                gr.update(label=get_text("temperature"), info=get_text("temperature_info")),  # temperature_slider
                gr.update(label=get_text("top_p"), info=get_text("top_p_info")),  # top_p_slider
                gr.update(label=get_text("top_k"), info=get_text("top_k_info")),  # top_k_slider
                gr.update(label=get_text("seed"), info=get_text("seed_info")),  # seed_number
                gr.update(value=get_text("random_seed_btn")),  # random_seed_btn
                gr.update(label=get_text("single_processing")),  # single_tab
                gr.update(label=get_text("upload_image")),  # single_image
                gr.update(label=get_text("image_url"), placeholder=get_text("image_url_placeholder")),  # single_image_url
                gr.update(label=get_text("prompt"), placeholder=get_text("prompt_placeholder")),  # single_prompt
                gr.update(value=get_text("generate_btn")),  # single_submit_btn
                gr.update(label=get_text("result")),  # single_output
                f"### {get_text('examples_title')}",  # examples_title
                gr.update(samples=update_examples()),  # single_examples
                gr.update(label=get_text("batch_processing")),  # batch_tab
                gr.update(label=get_text("upload_images")),  # batch_images
                gr.update(label=get_text("prompts_multiline"), placeholder=get_text("prompts_placeholder"), info=get_text("prompts_info")),  # batch_prompts
                gr.update(value=get_text("process_batch_btn")),  # batch_submit_btn
                gr.update(label=get_text("results")),  # batch_output
            ]

        language_dropdown.change(
            fn=change_language,
            inputs=language_dropdown,
            outputs=[
                header_md,
                model_dropdown,
                quantization_dropdown,
                language_dropdown,
                advanced_accordion,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number,
                random_seed_btn,
                single_tab,
                single_image,
                single_image_url,
                single_prompt,
                single_submit_btn,
                single_output,
                examples_title,
                single_examples,
                batch_tab,
                batch_images,
                batch_prompts,
                batch_submit_btn,
                batch_output,
            ]
        )

        single_examples.click(
            fn=lambda x: x[0] if x else "",
            inputs=[single_examples],
            outputs=[single_prompt]
        )
        
        random_seed_btn.click(
            fn=random_seed,
            outputs=seed_number
        )
        
        single_submit_btn.click(
            fn=process_single_image,
            inputs=[
                single_image,
                single_image_url,
                single_prompt,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=single_output
        )

        batch_submit_btn.click(
            fn=process_batch_images,
            inputs=[
                batch_images,
                batch_prompts,
                model_dropdown,
                quantization_dropdown,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                top_k_slider,
                seed_number
            ],
            outputs=batch_output
        )
        
        return demo

# –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
demo = create_interface()

if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True,
        inbrowser=True
    )
